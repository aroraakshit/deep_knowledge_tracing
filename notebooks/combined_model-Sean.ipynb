{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model that takes the complete sequence and the evaluation calculates AUCs on separate task IDs. <br>\n",
    "Evaluation method stays the same as separate_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint as pp\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can work with tf.VERSION = '1.4.1' (for MacOS High Sierra); functions may change for other versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'blue':'green'}\n",
    "save_obj(a,'a')\n",
    "training_set_split = 0.8\n",
    "validation_set_split = 0.1\n",
    "#learning_rate = np.logspace(-5,0,5)\n",
    "learning_rate = [0.01]\n",
    "num_units = 50 #number of units in RNN cell\n",
    "training_steps = 1000 #number of epochs (when convergence stopping not active)\n",
    "display_step = 20 #number of epochs after which to display progress\n",
    "optimize_using = \"adam\" #other option: \"momentum\", \"adagrad\", \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \" + str(tf.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading JSON file into dictionary called 'student_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath2 = \"../data/student_vectors_n_task_10_n_limit_100000.json\"\n",
    "student_vectors = json.load(open(filepath2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting unique CCSSM labels and Task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = []\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        if j['task_id'] not in task_ids:\n",
    "            task_ids.append(j['task_id'])\n",
    "print(\"Number of unique task IDs: \" + str(len(task_ids)))\n",
    "print(\"Number of students: \" + str(len(student_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 1-hot encoding for Task IDs and CCSSM Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for using MultiLabelBinarizer\n",
    "temp_ids = []\n",
    "for i in task_ids:\n",
    "    temp_ids.append([i])\n",
    "\n",
    "#generating encodings\n",
    "enc = MultiLabelBinarizer()\n",
    "task_ids_1hot = (enc.fit_transform(temp_ids)).astype(float)\n",
    "task_ids_classes = enc.classes_\n",
    "task_ids_dict = dict(zip(task_ids, task_ids_1hot))\n",
    "# print(\"\\n1-hot encoding for task IDs:\")\n",
    "# pp.pprint(task_ids_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating input sequences of interactions to feed the network. Say we have 3 task IDs; here is an example of interaction vectors generated:\n",
    "1. User correctly solves task 2 of label 3: [010   000]\n",
    "2. User incorrectly solves task 1 of label 2: [000   100]\n",
    "\n",
    "1-hot representation of task IDs: \n",
    "task ID 1: 1,0,0\n",
    "task ID 2: 0,1,0\n",
    "task ID 3: 0,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "output_y = []\n",
    "seqlen = []\n",
    "sequences_lengths1 = [] #for passing to mask\n",
    "incorrect_tid_vec = np.zeros((len(task_ids)), dtype=np.float)\n",
    "for i in student_vectors:\n",
    "    temp_seq = []\n",
    "    temp_seq.append(np.concatenate([incorrect_tid_vec,incorrect_tid_vec])) #for taking first prediction into account\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['second_try'] == False): #ignoring second_try\n",
    "            if(j['correct'] == True):\n",
    "                vec = np.concatenate([task_ids_dict[j['task_id']],incorrect_tid_vec])\n",
    "                temp_seq.append(vec)\n",
    "            else:\n",
    "                vec = np.concatenate([incorrect_tid_vec,task_ids_dict[j['task_id']]])\n",
    "                temp_seq.append(vec)\n",
    "    if(len(temp_seq)>1):\n",
    "        seqlen.append(len(temp_seq)-1)\n",
    "        sequences_lengths1.append([len(temp_seq)-1]) \n",
    "        last_one = temp_seq.pop() #remove last interaction vector\n",
    "        sequences.append(temp_seq) #add it to x\n",
    "        first_one = temp_seq.pop(0) #remove first interaction vector\n",
    "        temp_seq.append(last_one)\n",
    "        output_y.append(temp_seq) #concatenate with last vector, and append to output! \n",
    "    \n",
    "# print(\"Sample interaction vector: \")\n",
    "# pp.pprint(sequences[0][0])\n",
    "length_interaction_vector = 2*(len(task_ids)) #length of interaction vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution of taskids\n",
    "cnt2 = Counter()\n",
    "another_2 = {}\n",
    "seqlen_tasks = {}\n",
    "temp_seqlen = {}\n",
    "position_2 = 1\n",
    "for i in student_vectors:\n",
    "    for k in temp_seqlen:\n",
    "        temp_seqlen[k] = 0\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['second_try'] == False):\n",
    "            if j['task_id'] not in another_2:\n",
    "                another_2[j['task_id']] = str(position_2)\n",
    "                position_2 = position_2 + 1\n",
    "            if j['task_id'] not in seqlen_tasks:\n",
    "                seqlen_tasks[j['task_id']] = []\n",
    "\n",
    "            if j['task_id'] not in temp_seqlen:\n",
    "                temp_seqlen[j['task_id']] = 1\n",
    "            else:\n",
    "                temp_seqlen[j['task_id']] += 1\n",
    "            cnt2[another_2[j['task_id']]] += 1\n",
    "    for k in seqlen_tasks:\n",
    "        seqlen_tasks[k].append(temp_seqlen[k])\n",
    "plt.bar(cnt2.keys(), cnt2.values())\n",
    "plt.title(\"Task IDs distribution\")\n",
    "plt.xlabel(\"Frequency of task ID across all students\")\n",
    "plt.ylabel(\"Task IDs\")\n",
    "plt.show()\n",
    "print(\"Task IDs mapping: \")\n",
    "for i in another_2:\n",
    "    print(\"Task ID -> \"+str(i)+\"(\"+str(another_2[i])+\") is attempted \" + str(cnt2[another_2[i]]) + \" times.\" + \" Max seq len: \"+str(max(seqlen_tasks[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = max(seqlen)\n",
    "print(\"Maximum sequence length: \"+str(max_seqlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding the sequences according to maximum sequence length. Making padded sequences of shape: number of students, maximum sequence length, length of interaction vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = np.zeros(shape=(len(student_vectors),max_seqlen,length_interaction_vector),dtype=float)\n",
    "for i in range(len(sequences)):\n",
    "    for j in range(len(sequences[i])):\n",
    "        padded_sequences[i][j] = sequences[i][j]\n",
    "print(\"Sequences have been padded according to the maximum sequence length. Final shape: \" + str(padded_sequences.shape))\n",
    "\n",
    "padded_output = np.zeros(shape=(len(student_vectors),max_seqlen,length_interaction_vector),dtype=float)\n",
    "for i in range(len(output_y)):\n",
    "    for j in range(len(output_y[i])):\n",
    "        padded_output[i][j] = output_y[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets. The code for random split is ready.<br>\n",
    "Review: **Try using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split = round((training_set_split+validation_set_split)*len(student_vectors))\n",
    "split = int((training_set_split)*len(student_vectors))\n",
    "\n",
    "tmp_rnd = 0\n",
    "training_x = np.zeros_like(padded_sequences[:split])\n",
    "training_y = np.zeros_like(np.asarray(padded_output)[:split])\n",
    "training_seqlen = np.zeros_like(seqlen[:split])\n",
    "\n",
    "test_x = np.zeros_like(padded_sequences[split:])\n",
    "test_y = np.zeros_like(np.asarray(padded_output)[split:])\n",
    "test_seqlen = np.zeros_like(seqlen[split:])\n",
    "\n",
    "itr = 0\n",
    "itr_tr = 0\n",
    "itr_te = 0\n",
    "# import random\n",
    "# #separating training and testing sets randomly\n",
    "# for i in range(len(padded_sequences)):\n",
    "#     if(random.uniform(0, 1) <= (training_set_split+validation_set_split) and itr_tr < split):\n",
    "#         #add to training\n",
    "#         training_x[itr_tr] = padded_sequences[itr]\n",
    "#         training_y[itr_tr] = np.asarray(padded_output)[itr]\n",
    "#         training_y_taskid[itr_tr] = np.asarray(padded_output_taskid)[itr]\n",
    "#         training_seqlen[itr_tr] = seqlen[itr]\n",
    "#         itr_tr = itr_tr + 1\n",
    "#     elif(itr_te < (len(student_vectors) - split)):\n",
    "#         #add to testing\n",
    "#         test_x[itr_te] = padded_sequences[itr]\n",
    "#         test_y[itr_te] = np.asarray(padded_output)[itr]\n",
    "#         test_y_taskid[itr_te] = np.asarray(padded_output_taskid)[itr]\n",
    "#         test_seqlen[itr_te] = seqlen[itr]\n",
    "#         itr_te = itr_te + 1\n",
    "#     else:\n",
    "#         #add to training\n",
    "#         training_x[itr_tr] = padded_sequences[itr]\n",
    "#         training_y[itr_tr] = np.asarray(padded_output)[itr]\n",
    "#         training_y_taskid[itr_tr] = np.asarray(padded_output_taskid)[itr]\n",
    "#         training_seqlen[itr_tr] = seqlen[itr]\n",
    "#         itr_tr = itr_tr + 1\n",
    "#     itr = itr + 1\n",
    "\n",
    "#separating training & validation set\n",
    "training_x = padded_sequences[:split]\n",
    "training_y = np.asarray(padded_output)[:split]\n",
    "training_seqlen = seqlen[:split]\n",
    "\n",
    "#separating test set\n",
    "test_x = padded_sequences[split:]\n",
    "test_y = np.asarray(padded_output)[split:]\n",
    "test_seqlen = seqlen[split:]\n",
    "\n",
    "# #generating validation and training sets by implementing k-fold cross validation (k = maximum_position)\n",
    "# validation_set_size = math.floor(validation_set_split * len(student_vectors))\n",
    "# training_set_size = len(training_x) - validation_set_size\n",
    "# maximum_position = math.floor(len(training_x) / validation_set_size)\n",
    "\n",
    "# def get_next_train_valid_set(position):\n",
    "#     if(position>=maximum_position):\n",
    "#         position = position % maximum_position\n",
    "#     print(\"Picking validation set from position: \"+str(position))\n",
    "#     valid_start = position*validation_set_size\n",
    "#     valid_end = valid_start + validation_set_size\n",
    "    \n",
    "#     valid_set_x = training_x[valid_start : valid_end]\n",
    "#     valid_set_y = training_y[valid_start : valid_end]\n",
    "#     valid_set_y_taskid = np.asarray(training_y_taskid)[valid_start : valid_end]\n",
    "#     valid_set_seqlen = np.asarray(training_seqlen[valid_start:valid_end])\n",
    "    \n",
    "#     train_set_x = np.concatenate((training_x[:valid_start], training_x[valid_end:]))\n",
    "#     train_set_y = np.concatenate((training_y[:valid_start], training_y[valid_end:]))\n",
    "#     train_set_y_taskid = np.concatenate((np.asarray(training_y_taskid)[:valid_start], np.asarray(training_y_taskid)[valid_end:]))\n",
    "#     train_set_seqlen = np.concatenate((np.asarray(training_seqlen)[:valid_start],np.asarray(training_seqlen)[valid_end:]))\n",
    "    \n",
    "#     if(len(train_set_x) != training_set_size): #test\n",
    "#         print(\"that's not good it is:\")\n",
    "#         print(train_set_x.shape)\n",
    "    \n",
    "#     return (train_set_seqlen,valid_set_seqlen,valid_set_x,valid_set_y,valid_set_y_taskid,train_set_x,train_set_y,train_set_y_taskid)\n",
    "\n",
    "# print(\"Splitting \"+str(len(student_vectors))+\" rows randomly into \"+str(training_set_size)+ \" for training, \"+str(validation_set_size)+\" for validation and \"+str(len(test_x)) + \" for testing.\")\n",
    "# print(\"Implemented \"+str(maximum_position)+\"-fold cross validation.\")\n",
    "print(\"Splitting \"+str(len(student_vectors))+\" rows (or students) into \"+str(len(training_x))+ \" for training and \"+str(len(test_x)) + \" for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1234)\n",
    "#defining placeholders\n",
    "x = tf.placeholder(tf.float32, [None, max_seqlen, length_interaction_vector])\n",
    "y = tf.placeholder(tf.float32, [None, max_seqlen, length_interaction_vector])\n",
    "seqlen_tf = tf.placeholder(tf.float32,[None])\n",
    "condition = tf.placeholder(tf.int32, shape=[], name=\"condition\")\n",
    "\n",
    "#defining tensorflow variables\n",
    "learning_tf_rate = tf.Variable(0.0, name=\"learning_tf_rate\",dtype=tf.float32,trainable=False)\n",
    "\n",
    "#dynamic RNN definition\n",
    "def dynamicRNN(x):\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32,sequence_length=seqlen_tf)\n",
    "    out_size = int(length_interaction_vector / 2)\n",
    "    outputs = tf.contrib.layers.fully_connected(outputs, out_size, activation_fn = tf.nn.sigmoid, weights_initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    opposites = tf.subtract(tf.ones(tf.shape(outputs)),outputs)\n",
    "    outputs1 = tf.concat([outputs,opposites],2)\n",
    "    return outputs1\n",
    "\n",
    "#making predictions\n",
    "pred = dynamicRNN(x)\n",
    "pred = pred*y\n",
    "# Define loss and optimizer\n",
    "cost1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "mask = tf.cond(condition < 1, lambda: tf.cast(tf.sequence_mask(lengths=sequences_lengths1[:split], maxlen = max_seqlen), tf.float32), lambda: tf.cast(tf.sequence_mask(lengths=sequences_lengths1[split:], maxlen = max_seqlen), tf.float32))\n",
    "cost1 = tf.multiply(cost1,tf.transpose(mask, perm=[0, 2, 1]))\n",
    "cost1 = tf.reduce_sum(cost1, 1)\n",
    "cost1 /= tf.cond(condition < 1, lambda: tf.cast(sequences_lengths1[:split],tf.float32), lambda: tf.cast(sequences_lengths1[split:],tf.float32) )\n",
    "cost = tf.reduce_mean(cost1)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_tf_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning test set into separate task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_list_dict = {}\n",
    "test_y_list_dict = {}\n",
    "test_seqlen_list_dict = {}\n",
    "train_x_list_dict = {}\n",
    "train_y_list_dict = {}\n",
    "train_seqlen_list_dict = {}\n",
    "split_dict = {}\n",
    "for i in another_2: \n",
    "    #generate encoding here! \n",
    "    sequences = []\n",
    "    sequences_lengths = []\n",
    "    for p in student_vectors:\n",
    "        interactions = []\n",
    "        interactions.append(np.concatenate([incorrect_tid_vec,incorrect_tid_vec])) #for getting the first prediction!\n",
    "        for j in student_vectors[p]:\n",
    "            if(j['task_id'] == i and j['second_try'] == False):\n",
    "                if(j['correct'] == True):\n",
    "                    vec = np.concatenate([task_ids_dict[j['task_id']],incorrect_tid_vec])\n",
    "                    interactions.append(vec)\n",
    "                else:\n",
    "                    interactions.append(np.concatenate([incorrect_tid_vec,task_ids_dict[j['task_id']]]))\n",
    "        if(len(interactions) > 1):\n",
    "            sequences_lengths.append(len(interactions)-1)\n",
    "            sequences.append(interactions)\n",
    "    split = int(0.8*len(sequences))\n",
    "    #add padding\n",
    "    padded_sequences = np.zeros([len(sequences),max_seqlen+1,length_interaction_vector])\n",
    "    for p in range(len(sequences)):\n",
    "        for j in range(len(sequences[p])):\n",
    "            padded_sequences[p][j] = sequences[p][j]\n",
    "    test_x_list_dict[i] = padded_sequences[split:,:-1]\n",
    "    test_y_list_dict[i] = padded_sequences[split:,1:]\n",
    "    test_seqlen_list_dict[i] = sequences_lengths[split:]\n",
    "    train_x_list_dict[i] = padded_sequences[:split,:-1]\n",
    "    train_y_list_dict[i] = padded_sequences[:split,1:]\n",
    "    train_seqlen_list_dict[i] = sequences_lengths[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "def calculate_auc (y_true,y_pred,sequence_lengths=[],plot=False,debug=False):\n",
    "    if sequence_lengths == []:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        con_y_true = np.zeros([sum(sequence_lengths),length_interaction_vector])\n",
    "        con_y_pred = np.zeros([sum(sequence_lengths),length_interaction_vector])\n",
    "        index = 0\n",
    "        for i in range(len(y_true)): #per student\n",
    "            for j in range(sequence_lengths[i]): #up to the sequence length\n",
    "                con_y_true[index] = y_true[i][j]\n",
    "                con_y_pred[index] = y_pred[i][j]\n",
    "                index += 1\n",
    "        con1_y_true = np.zeros([sum(sequence_lengths)])\n",
    "        con1_y_pred = np.zeros([sum(sequence_lengths)])\n",
    "        \n",
    "        for l in range(sum(sequence_lengths)):\n",
    "            index_one = np.argmax(con_y_true[l])\n",
    "            if(index_one >= int(length_interaction_vector/2)):\n",
    "                index_two = index_one - int(length_interaction_vector/2)\n",
    "            else:\n",
    "                index_two = index_one\n",
    "                index_one = index_one + int(length_interaction_vector/2)\n",
    "            if(np.argmax(con_y_true[l]) == index_one): #true is incorrect\n",
    "                con1_y_true[l] = 0.\n",
    "                con1_y_pred[l] = 1.0 - con_y_pred[l][index_one]\n",
    "            elif(np.argmax(con_y_true[l]) == index_two):\n",
    "                con1_y_true[l] = 1.\n",
    "                con1_y_pred[l] = con_y_pred[l][index_two]\n",
    "        debug=False\n",
    "        if(debug):\n",
    "            print(np.c_[con1_y_true,con1_y_pred])\n",
    "        fpr, tpr, thresholds = roc_curve(con1_y_true, con1_y_pred)\n",
    "        #print(\"tpr: \"+str(tpr) + \", fpr: \"+str(fpr) + \", thresholds: \"+str(thresholds))\n",
    "        if(plot):\n",
    "            return [roc_auc_score(con1_y_true, con1_y_pred),fpr,tpr]\n",
    "        else:\n",
    "            return roc_auc_score(con1_y_true, con1_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_all_tasks = {}\n",
    "true_all_tasks['train'] = {}\n",
    "true_all_tasks['test'] = {}\n",
    "predictions_all_tasks = {}\n",
    "predictions_all_tasks['train'] = {}\n",
    "predictions_all_tasks['test'] = {}\n",
    "seqlen_all_tasks = {}\n",
    "seqlen_all_tasks['train'] = {}\n",
    "seqlen_all_tasks['test'] = {}\n",
    "for i in another_2:\n",
    "    seqlen_all_tasks['train'][i] = []\n",
    "    seqlen_all_tasks['test'][i] = []\n",
    "    predictions_all_tasks['train'][i] = []\n",
    "    predictions_all_tasks['test'][i] = []\n",
    "    true_all_tasks['train'][i] = []\n",
    "    true_all_tasks['test'][i] = []\n",
    "seqlen_all_tasks['train']['overall'] = []\n",
    "seqlen_all_tasks['test']['overall'] = []\n",
    "predictions_all_tasks['train']['overall'] = []\n",
    "predictions_all_tasks['test']['overall'] = []\n",
    "true_all_tasks['train']['overall'] = []\n",
    "true_all_tasks['test']['overall'] = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for l_r in learning_rate:\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        assign_op = learning_tf_rate.assign(l_r)\n",
    "        sess.run(assign_op)\n",
    "        cost_prev = 1.0\n",
    "        stop = False\n",
    "        #print(\"Final Learning Rate: \"+str(learning_tf_rate.eval()))\n",
    "        step = 0\n",
    "        while(stop==False):\n",
    "        #for step in range(1, training_steps):\n",
    "            sess.run(optimizer, feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "            step += 1\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                loss = sess.run(cost, feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "                cost_current = loss\n",
    "                if cost_prev - cost_current <= 0.00005:\n",
    "                    stop = True\n",
    "                else:\n",
    "                    cost_prev = cost_current\n",
    "    \n",
    "                #calculate overall training AUC\n",
    "                pred_train,train_cost = sess.run([pred,cost], feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "                o_train_opts_taskid = calculate_auc(training_y,pred_train,training_seqlen)\n",
    "                true_all_tasks['train']['overall'].append(training_y)\n",
    "                predictions_all_tasks['train']['overall'].append(pred_train)\n",
    "                seqlen_all_tasks['train']['overall'].append(o_train_opts_taskid)\n",
    "                # Calculate overall test auc\n",
    "                pred_test,test_cost = sess.run([pred,cost],feed_dict={x: test_x, y: test_y, seqlen_tf: test_seqlen,condition:1})\n",
    "                o_temp_opts_taskid = calculate_auc(test_y,pred_test,test_seqlen)\n",
    "                true_all_tasks['test']['overall'].append(test_y)\n",
    "                predictions_all_tasks['test']['overall'].append(pred_test)\n",
    "                seqlen_all_tasks['test']['overall'].append(o_temp_opts_taskid)\n",
    "                #print status\n",
    "                print(\"Epoch \" + str(step) + \", Train loss = \" + str(train_cost) + \", Train AUC = \"+str(o_train_opts_taskid) +\", Test Loss: \"+str(test_cost)+ \", Test AUC: \"+str(o_temp_opts_taskid))\n",
    "                \n",
    "                #calculate train and test AUCs on task IDs separately!\n",
    "                for idx,i in enumerate(another_2):\n",
    "                    pred_train = sess.run(pred, feed_dict={x: train_x_list_dict[i], y: train_y_list_dict[i], seqlen_tf: train_seqlen_list_dict[i] ,condition:0})\n",
    "                    if(i=='9wRCzK1G7F.partb' or i=='1zsCldT4p8.set2'):\n",
    "                        train_opts_taskid = calculate_auc(train_y_list_dict[i],pred_train,train_seqlen_list_dict[i],debug=True)\n",
    "                    else:\n",
    "                        train_opts_taskid = calculate_auc(train_y_list_dict[i],pred_train,train_seqlen_list_dict[i])\n",
    "                    true_all_tasks['train'][i].append(train_y_list_dict[i])\n",
    "                    predictions_all_tasks['train'][i].append(pred_train)\n",
    "                    seqlen_all_tasks['train'][i].append(train_opts_taskid)\n",
    "                    pred_test = sess.run(pred,feed_dict={x: test_x_list_dict[i], y: test_y_list_dict[i], seqlen_tf: test_seqlen_list_dict[i] ,condition:1})\n",
    "                    if(i=='9wRCzK1G7F.partb' or i=='1zsCldT4p8.set2'):\n",
    "                        temp_opts_taskid = calculate_auc(test_y_list_dict[i],pred_test,test_seqlen_list_dict[i],debug=True)\n",
    "                    else:\n",
    "                        temp_opts_taskid = calculate_auc(test_y_list_dict[i],pred_test,test_seqlen_list_dict[i])\n",
    "                    true_all_tasks['test'][i].append(test_y_list_dict[i])\n",
    "                    predictions_all_tasks['test'][i].append(pred_test)\n",
    "                    seqlen_all_tasks['test'][i].append(temp_opts_taskid)\n",
    "                    print(str(i) + \" -> Train AUC = \"+str(train_opts_taskid) + \", Test AUC: \"+str(temp_opts_taskid))\n",
    "                \n",
    "#                 view_point = np.argsort(test_seqlen)[1]\n",
    "#                 prediction = sess.run(pred, feed_dict={x: [test_x[view_point]], y: [test_y[view_point]], seqlen_tf: [test_seqlen[view_point]],condition:1})\n",
    "#                 print(\"\\n\\ntrue:prediction\\n\")\n",
    "#                 print(np.c_[test_x[view_point][:test_seqlen[view_point]],prediction[0][:test_seqlen[view_point]]])\n",
    "                print(\"\\n\\n\")\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "        for idx,i in enumerate(another_2):\n",
    "            pred_test = sess.run(pred,feed_dict={x: test_x_list_dict[i], y: test_y_list_dict[i], seqlen_tf: test_seqlen_list_dict[i] ,condition:1})\n",
    "            true_all_tasks['test'][i].append(test_y_list_dict[i])\n",
    "            predictions_all_tasks['test'][i].append(pred_test)\n",
    "            seqlen_all_tasks['test'][i].append(test_seqlen_list_dict[i])\n",
    "            pred_train = sess.run(pred, feed_dict={x: train_x_list_dict[i], y: train_y_list_dict[i], seqlen_tf: train_seqlen_list_dict[i] ,condition:0})\n",
    "            true_all_tasks['train'][i].append(train_y_list_dict[i])\n",
    "            predictions_all_tasks['train'][i].append(pred_train)\n",
    "            seqlen_all_tasks['train'][i].append(train_seqlen_list_dict[i])\n",
    "        \n",
    "        pred_test,test_cost = sess.run([pred,cost],feed_dict={x: test_x, y: test_y, seqlen_tf: test_seqlen,condition:1})\n",
    "        true_all_tasks['test']['overall'].append(test_y)\n",
    "        predictions_all_tasks['test']['overall'].append(pred_test)\n",
    "        seqlen_all_tasks['test']['overall'].append(test_seqlen)\n",
    "        pred_train,train_cost = sess.run([pred,cost], feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "        true_all_tasks['train']['overall'].append(training_y)\n",
    "        predictions_all_tasks['train']['overall'].append(pred_train)\n",
    "        seqlen_all_tasks['train']['overall'].append(training_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating over-all AUC\")\n",
    "o_train = calculate_auc(true_all_tasks['train']['overall'][-1],predictions_all_tasks['train']['overall'][-1],seqlen_all_tasks['train']['overall'][-1],plot=False)\n",
    "o_test = calculate_auc(true_all_tasks['test']['overall'][-1],predictions_all_tasks['test']['overall'][-1],seqlen_all_tasks['test']['overall'][-1],plot=False)\n",
    "print(\"Overall AUC train: \"+str(o_train) + \", test: \"+str(o_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in another_2:\n",
    "    print(i + \" final train acc: \" + str(round(seqlen_all_tasks['train'][i][-2],3)) + \"; final test acc: \"+str(round(seqlen_all_tasks['test'][i][-2],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(14, 9), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Bayesian Knowledge Tracing (combined model, test set)')\n",
    "\n",
    "# for task_id in another_2:\n",
    "#     roc = calculate_auc(true_all_tasks['test'][task_id][-1] , predictions_all_tasks['test'][task_id][-1] , seqlen_all_tasks['test'][task_id][-1],plot=True)\n",
    "#     plt.plot(roc[1], roc[2], label=task_id)\n",
    "\n",
    "# roc = calculate_auc(true_all_tasks['test']['overall'][-1],predictions_all_tasks['test']['overall'][-1],seqlen_all_tasks['test']['overall'][-1],plot=True)\n",
    "# plt.plot(roc[1], roc[2], label='overall-test')\n",
    "\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('combined1.png')\n",
    "save_obj(true_all_tasks,\"true_all_tasks\")\n",
    "save_obj(predictions_all_tasks,\"predictions_all_tasks\")\n",
    "save_obj(seqlen_all_tasks,\"seqlen_all_tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(14, 9), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Bayesian Knowledge Tracing (combined model, Train set)')\n",
    "\n",
    "# for task_id in another_2:\n",
    "#     roc = calculate_auc(true_all_tasks['train'][task_id][-1],predictions_all_tasks['train'][task_id][-1], seqlen_all_tasks['train'][task_id][-1], plot=True)\n",
    "#     plt.plot(roc[1], roc[2], label=task_id)\n",
    "\n",
    "# roc = calculate_auc(true_all_tasks['train']['overall'][-1],predictions_all_tasks['train']['overall'][-1],seqlen_all_tasks['train']['overall'][-1],plot=True)\n",
    "# plt.plot(roc[1], roc[2], label='overall-train')\n",
    "\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('combined2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(6,2, figsize=(20, 30), facecolor='w', edgecolor='k')\n",
    "# fig.subplots_adjust(hspace = .5, wspace=0.1)\n",
    "\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i,value in enumerate(another_2):\n",
    "#     train_auc = seqlen_all_tasks['train'][value][:-1]\n",
    "#     test_auc = seqlen_all_tasks['test'][value][:-1]\n",
    "#     axs[i-1].plot(test_auc,label='test')\n",
    "#     axs[i-1].plot(train_auc,label='train')\n",
    "#     axs[i-1].set_ylim(0,1.1)\n",
    "#     axs[i-1].set_xlim(0,12)\n",
    "#     axs[i-1].set_xlabel(\"Epochs\")\n",
    "#     axs[i-1].set_ylabel(\"AUC\")\n",
    "#     axs[i-1].legend(loc=\"best\")\n",
    "#     axs[i-1].set_title(\"Task ID = \"+str(value))\n",
    "    \n",
    "# train_auc = seqlen_all_tasks['train']['overall'][:-1]\n",
    "# test_auc = seqlen_all_tasks['test']['overall'][:-1]\n",
    "# axs[i].plot(test_auc,label='test')\n",
    "# axs[i].plot(train_auc,label='train')\n",
    "# axs[i].set_ylim(0,1.1)\n",
    "# axs[i].set_xlim(0,12)\n",
    "# axs[i].set_xlabel(\"Epochs\")\n",
    "# axs[i].set_ylabel(\"AUC\")\n",
    "# axs[i].legend(loc=\"best\")\n",
    "# axs[i].set_title(\"Overall\")\n",
    "# plt.savefig('combined3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No longer maintained below this point.**<br>Training the model for hyperparameter (learning rate) tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_lr = []\n",
    "# plot_valid_auc_taskid = []\n",
    "# plot_train_auc_taskid = []\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     for l_r in learning_rate:\n",
    "#         plot_lr.append(l_r)    \n",
    "#         valid_taskid_list = []\n",
    "#         for k_fold in range(1,maximum_position+1):\n",
    "#             # Initialize the variables (i.e. assign their default value)\n",
    "#             tf.reset_default_graph()\n",
    "#             print(str(k_fold)+\"-fold cross-validation\")\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "#             sess.run(tf.local_variables_initializer())\n",
    "#             assign_op = learning_tf_rate.assign(l_r)\n",
    "#             sess.run(assign_op)\n",
    "#             print(\"Current Learning Rate: \"+str(learning_tf_rate.eval()))\n",
    "#             train_set_seqlen, valid_set_seqlen, valid_set_x, valid_set_y, valid_set_y_taskid, train_set_x, train_set_y, train_set_y_taskid = get_next_train_valid_set(k_fold-1)\n",
    "            \n",
    "#             for step in range(1, training_steps+1):\n",
    "#                 batch_x = train_set_x\n",
    "#                 batch_y = train_set_y\n",
    "#                 sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, y_taskid: train_set_y_taskid, seqlen_tf: train_set_seqlen})\n",
    "\n",
    "#                 if step % display_step == 0 or step == 1:\n",
    "#                     loss,trainAUC,trainOPTS = sess.run([cost, auc, opts], feed_dict={x: batch_x, y: batch_y, y_taskid: train_set_y_taskid, seqlen_tf: train_set_seqlen})\n",
    "#                     #print status\n",
    "#                     print(\"Step \" + str(step) + \", Loss = \" + str(loss) + \", Learning Rate = \"+str(learning_tf_rate.eval()) + \", Train AUC:\" + str(trainOPTS))\n",
    "#             #calculate validation AUC\n",
    "#             valid_auc_taskid, valid_opts_taskid = sess.run([auc, opts], feed_dict={x: valid_set_x, y: valid_set_y, y_taskid: valid_set_y_taskid, seqlen_tf: valid_set_seqlen})\n",
    "#             print(\"Valid_auc_taskid: \" + str(valid_opts_taskid) + \" with k = \"+str(k_fold))\n",
    "#             valid_taskid_list.append(valid_opts_taskid)\n",
    "#             print(\"Optimization Finished!\")\n",
    "    \n",
    "#         #calculate training AUC (it should take both validation and training sets)\n",
    "#         train_auc_taskid, train_opts_taskid = sess.run([auc, opts], feed_dict={x: training_x, y: training_y, y_taskid: training_y_taskid, seqlen_tf: training_seqlen})\n",
    "\n",
    "#         print(\"Train_auc_taskid: \" + str(train_opts_taskid))\n",
    "#         plot_train_auc_taskid.append(train_opts_taskid)\n",
    "        \n",
    "#         #take average of validation AUCs\n",
    "#         valid_avg_taskid = np.mean(valid_taskid_list)\n",
    "        \n",
    "#         print(\"Average Valid_auc_taskid: \" + str(valid_avg_taskid))\n",
    "#         plot_valid_auc_taskid.append(valid_avg_taskid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting validation set ccssm auc across different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Training Set AUC\")\n",
    "# plt.xlabel(\"Learning Rate Index\")\n",
    "# plt.ylabel(\"AUC\")\n",
    "# plt.plot(plot_train_auc_taskid,'r--',label='train')\n",
    "# plt.plot(plot_valid_auc_taskid,label='valid')\n",
    "# plt.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "# plt.show()\n",
    "# print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
