{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model that takes the complete sequence and the evaluation calculates AUCs on separate task IDs. <br>\n",
    "Evaluation method stays the same as separate_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint as pp\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can work with tf.VERSION = '1.4.1' (for MacOS High Sierra); functions may change for other versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_split = 0.8\n",
    "validation_set_split = 0.1\n",
    "#learning_rate = np.logspace(-5,0,5)\n",
    "learning_rate = [0.01]\n",
    "num_units = 50 #number of units in RNN cell\n",
    "training_steps = 1000 #number of epochs (when convergence stopping not active)\n",
    "display_step = 20 #number of epochs after which to display progress\n",
    "optimize_using = \"adam\" #other option: \"momentum\", \"adagrad\", \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \" + str(tf.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading JSON file into dictionary called 'student_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/student_vectors_n_task_10_n_limit_10000.json\"\n",
    "filepath2 = \"../../../student_vectors_n_task_10_n_limit_100000.json\"\n",
    "student_vectors2 = json.load(open(filepath))\n",
    "student_vectors = json.load(open(filepath2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting unique CCSSM labels and Task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique task IDs: 10\n",
      "Number of students: 11659\n"
     ]
    }
   ],
   "source": [
    "task_ids = []\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        if j['task_id'] not in task_ids:\n",
    "            task_ids.append(j['task_id'])\n",
    "print(\"Number of unique task IDs: \" + str(len(task_ids)))\n",
    "print(\"Number of students: \" + str(len(student_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 1-hot encoding for Task IDs and CCSSM Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for using MultiLabelBinarizer\n",
    "temp_ids = []\n",
    "for i in task_ids:\n",
    "    temp_ids.append([i])\n",
    "\n",
    "#generating encodings\n",
    "enc = MultiLabelBinarizer()\n",
    "task_ids_1hot = (enc.fit_transform(temp_ids)).astype(float)\n",
    "task_ids_classes = enc.classes_\n",
    "task_ids_dict = dict(zip(task_ids, task_ids_1hot))\n",
    "# print(\"\\n1-hot encoding for task IDs:\")\n",
    "# pp.pprint(task_ids_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating input sequences of interactions to feed the network. Say we have 3 task IDs; here is an example of interaction vectors generated:\n",
    "1. User correctly solves task 2 of label 3: [010   000]\n",
    "2. User incorrectly solves task 1 of label 2: [000   100]\n",
    "\n",
    "1-hot representation of task IDs: \n",
    "task ID 1: 1,0,0\n",
    "task ID 2: 0,1,0\n",
    "task ID 3: 0,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "output_y = []\n",
    "seqlen = []\n",
    "sequences_lengths1 = [] #for passing to mask\n",
    "incorrect_tid_vec = np.zeros((len(task_ids)), dtype=np.float)\n",
    "for i in student_vectors:\n",
    "    temp_seq = []\n",
    "    temp_seq.append(np.concatenate([incorrect_tid_vec,incorrect_tid_vec])) #for taking first prediction into account\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['second_try'] == False): #ignoring second_try\n",
    "            if(j['correct'] == True):\n",
    "                vec = np.concatenate([task_ids_dict[j['task_id']],incorrect_tid_vec])\n",
    "                temp_seq.append(vec)\n",
    "            else:\n",
    "                vec = np.concatenate([incorrect_tid_vec,task_ids_dict[j['task_id']]])\n",
    "                temp_seq.append(vec)\n",
    "    if(len(temp_seq)>1):\n",
    "        seqlen.append(len(temp_seq)-1)\n",
    "        sequences_lengths1.append([len(temp_seq)-1]) \n",
    "        last_one = temp_seq.pop() #remove last interaction vector\n",
    "        sequences.append(temp_seq) #add it to x\n",
    "        first_one = temp_seq.pop(0) #remove first interaction vector\n",
    "        temp_seq.append(last_one)\n",
    "        output_y.append(temp_seq) #concatenate with last vector, and append to output! \n",
    "    \n",
    "# print(\"Sample interaction vector: \")\n",
    "# pp.pprint(sequences[0][0])\n",
    "length_interaction_vector = 2*(len(task_ids)) #length of interaction vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xm8FOWd7/HPV3DfQGUYBQxGiY46rkclixkjI6JmgjNjjI6J6BhJJppRbyYJZjNqcsfEvDRxjN5xlAiJEbc4cuOCBHWyouCKuMQTXABRzg2K+4L+7h/1ay2Pfc5ppLrbI9/369WvrvrVU/U81Rz61/VU1VOKCMzMzKqwRrsbYGZm7x1OKmZmVhknFTMzq4yTipmZVcZJxczMKuOkYmZmlXFSsdWGpGmSvtHudnQn6QxJF+b0ByQ9XeG2L5b0lZweJ6mzwm3/raS7q9qevTc4qdi7jqTnSq/XJb1Ymj+iRW2YLenTOT0u21Frw0JJl0ratep6I+KPETGogfZ9XtKvGtjeURHx/VVtl6R1JIWk4aVt/yoidl7Vbdt7i5OKvetExAa1F/AY8Hel2CVtataCbM9GwIeAh4HfS9q7Te3pk6QB7W6DrX6cVKzfkfRhSbdKelrS45LOljQwlw2Q9GNJXZKWS7pb0rZ1trGxpN9KOnNl6o6I1yNiYUR8DbgE+PeVqTfLbiPpd5KelXQ9MLi0bDtJK0rzx0p6JMsukPTJPEL6IbBPHjk9kWWnSTpH0o2Sngc+WK/LT9KpkpZJeljSJ0vxN47Ocr58NPTrfH8w6zy4e3eapL+W9Jv8d7lH0gGlZdMk/VDSjNyX30l638p89tY/OKlYf/QqcDywKbA38HfAZ3PZx4HdgK0pvqz/CXiqvLKkvwBuAW6IiC+vQjt+AYyWtGYj9WbdAq6g+JLeFPgB8Jl6G5c0GDgTGBMRGwIfAe6NiDuBE4Fb8ujtL0urfRr4JrAhMKfOZkcCawF/CRwLTJG0VQP7+tF83zbr/O9ubV0HuBb4b2AI8GXgim7b/ifgZGATYAlwagP1Wj/jpGL9TkTcFhFzIuK1iPgTcCHwN7n4VYouqu2y7PyIWFpafUuKL/TJEfGdVWzK48CArK+vemtGAX8FnBoRr0TELOCGPurZUdI6EfF4RNzfR9krI+LWPKJ6uc7yFaW6fwX8Cjikj202Ym8ggLMi4tWImAHMBD5VKnN5RNwREa8CPwd2qaBee5dxUrF+R9L2kq6X9KSkZ4BvAZvl4uuBi4D/BJ6QdJ6kDUqrj6f48ptcQVOGAa8BzzRQb80WQFdEvFSKPVpv4xHxFHAE8K+5zemStumjTQv7WF6v7i36WKcRWwCPxVtHqH2U4jOqeaI0/QJQ7/Oxfs5Jxfqj/wLuALaOiI2A0wABROGsiNgV2AnYGTihtO65wO+B6ZLWXcV2/D0wO3+Z91VvzRJgs+wuqtmypwoi4tqIGEN+aQPn1xb1tEofba5X9+M5/TywXmlZuVutr+0+ztv3Y0tgcR/r2XuMk4r1RxsCyyPiOUk7UJwbAEDSaEkdeeL+eeAV4PXSupHlHwf+W9LaK1OxCsMlnU5x/uLrDdZb80fgQeCbktaS9DFgXA91DZN0kKT1gJeB50rbfBIYkedzVsaapbr3BfYDrspldwGH5OXD2wFH1VbKrrTlwPt72O5vgDUknShpoKT9gLHA5SvZPuvnnFSsPzoJ+Kyk54AfA5eVlg0CLgaeBhZQdMH8qLxyRLxO8YX5NHCVpLUaqPP9Wd9zwK3AtsBHIuJ/Gq036w7gUOBjwDLgK8DPeqhzADCJotvoz8AeFBcoQHEe5hFgqaRFDbS/5hGK8ypPUHQBHh0RC3LZ94GBQBdwQZ12fYvi5PvTkj7Rbb9eorhY4ZBs61nAp0rbttWE/JAuMzOrio9UzMysMk4qZmZWmaYmFUknSZov6V4VYyWtI2mrvBu6U9Jltf5sSWvnfGcuH1nazskZf1DS/qX4uIx1SprUzH0xM7O+NS2pSBpGcX19R0TsSHHS8TDge8DZEbENxR3Hx+QqxwBPZfzsLIek7XO9HSiukjlPxZAYAyhO0h4AbA8cnmXNzKxNBrZg++tKepXi+vclwL4UwzUATAG+TXHt/ficBrgSODeHtBgPTMtLGh/OsYb2zHKdtatLJE3Lsvf11qDNNtssRo4cWcW+mZmtFm6//fb/FxFDGinbtKQSEYsl/YDihq0XgRuB24GnI6I2YN4i3rzjdhh5N3BErJC0nGJspGHA7NKmy+ss7Bbfq15bJE0EJgJsueWWzJ07d9V2zsxsNSKp7qgP9TSz+2swxZHDVhR3A69PDzd5NVtEXBARHRHRMWRIQ8nWzMzegWaeqP9b4OGI6MoB5H4BfBgYlHcdAwznzWEcFgMjAHL5xhQ3Ub0R77ZOT3EzM2uTZiaVxyiGBV8vz42MoTjfcTNvjoo6Abgmp6fnPLn8prz7eDpwWF4dthXFKK+3UQzrPSqvJluL4mT+9Cbuj5mZ9aGZ51RulXQlxcB/K4A7KYZ+uBaYJuk7GbsoV7kI+GmeiF9GkSSIiPmSLqdISCuA4yLiNQBJxwMzKK4smxwR85u1P2Zm1rfVbpiWjo6O8Il6M7PGSbo9IjoaKes76s3MrDJOKmZmVhknFTMzq4yTipmZVabZw7S8p4ycdG3T63jkjIOaXkd/4s/crH/xkYqZmVXGScXMzCrj7i+zdyF3+1l/5SMVMzOrjJOKmZlVxknFzMwq46RiZmaVcVIxM7PKOKmYmVllnFTMzKwyTipmZlYZJxUzM6uMk4qZmVWmaUlF0raS7iq9npF0oqRNJM2U9FC+D87yknSOpE5J90jarbStCVn+IUkTSvHdJc3Ldc6RpGbtj5mZ9a1pSSUiHoyIXSJiF2B34AXgamASMCsiRgGzch7gAGBUviYC5wNI2gQ4BdgL2BM4pZaIssyxpfXGNWt/zMysb63q/hoD/CkiHgXGA1MyPgU4OKfHA1OjMBsYJGlzYH9gZkQsi4ingJnAuFy2UUTMjogAppa2ZWZmbdCqpHIYcGlOD42IJTn9BDA0p4cBC0vrLMpYb/FFdeJvI2mipLmS5nZ1da3KfpiZWS+anlQkrQV8Arii+7I8wohmtyEiLoiIjojoGDJkSLOrMzNbbbXiSOUA4I6IeDLnn8yuK/J9acYXAyNK6w3PWG/x4XXiZmbWJq1IKofzZtcXwHSgdgXXBOCaUvzIvApsNLA8u8lmAGMlDc4T9GOBGbnsGUmj86qvI0vbMjOzNmjqkx8lrQ/sB3yuFD4DuFzSMcCjwKEZvw44EOikuFLsaICIWCbpdGBOljstIpbl9BeAi4F1gevzZWZmbdLUpBIRzwObdov9meJqsO5lAziuh+1MBibXic8FdqyksWbWdn6Mcv/nO+rNzKwyTipmZlYZJxUzM6uMk4qZmVWmqSfqzcysb++lCxR8pGJmZpVxUjEzs8o4qZiZWWWcVMzMrDJOKmZmVhknFTMzq4yTipmZVcZJxczMKuObH83sLd5LN+JZ6/lIxczMKuOkYmZmlXFSMTOzyjipmJlZZZqaVCQNknSlpAck3S/pg5I2kTRT0kP5PjjLStI5kjol3SNpt9J2JmT5hyRNKMV3lzQv1zlHkpq5P2Zm1rtmH6n8CLghIrYDdgbuByYBsyJiFDAr5wEOAEblayJwPoCkTYBTgL2APYFTaokoyxxbWm9ck/fHzMx60bSkImlj4KPARQAR8UpEPA2MB6ZksSnAwTk9HpgahdnAIEmbA/sDMyNiWUQ8BcwExuWyjSJidkQEMLW0LTMza4NmHqlsBXQBP5F0p6QLJa0PDI2IJVnmCWBoTg8DFpbWX5Sx3uKL6sTfRtJESXMlze3q6lrF3TIzs540M6kMBHYDzo+IXYHnebOrC4A8wogmtqFWzwUR0RERHUOGDGl2dWZmq61mJpVFwKKIuDXnr6RIMk9m1xX5vjSXLwZGlNYfnrHe4sPrxM3MrE2allQi4glgoaRtMzQGuA+YDtSu4JoAXJPT04Ej8yqw0cDy7CabAYyVNDhP0I8FZuSyZySNzqu+jixty8zM2qDZY399EbhE0lrAAuBoikR2uaRjgEeBQ7PsdcCBQCfwQpYlIpZJOh2Yk+VOi4hlOf0F4GJgXeD6fFnFPBaUmTWqqUklIu4COuosGlOnbADH9bCdycDkOvG5wI6r2EwzM6uI76g3M7PKOKmYmVllnFTMzKwyTipmZlYZJxUzM6uMk4qZmVXGScXMzCrjpGJmZpVxUjEzs8o4qZiZWWWcVMzMrDJOKmZmVhknFTMzq4yTipmZVcZJxczMKuOkYmZmlXFSMTOzyjT7ccJWET/S18z6g6YeqUh6RNI8SXdJmpuxTSTNlPRQvg/OuCSdI6lT0j2SdittZ0KWf0jShFJ899x+Z66rZu6PmZn1rhXdXx+LiF0iovas+knArIgYBczKeYADgFH5mgicD0USAk4B9gL2BE6pJaIsc2xpvXHN3x0zM+tJO86pjAem5PQU4OBSfGoUZgODJG0O7A/MjIhlEfEUMBMYl8s2iojZERHA1NK2zMysDZqdVAK4UdLtkiZmbGhELMnpJ4ChOT0MWFhad1HGeosvqhN/G0kTJc2VNLerq2tV9sfMzHrR7BP1H4mIxZL+Apgp6YHywogISdHkNhARFwAXAHR0dDS9PjOz1VVTj1QiYnG+LwWupjgn8mR2XZHvS7P4YmBEafXhGestPrxO3MzM2qRpSUXS+pI2rE0DY4F7gelA7QquCcA1OT0dODKvAhsNLM9ushnAWEmD8wT9WGBGLntG0ui86uvI0rbMzKwNmtn9NRS4Oq/yHQj8PCJukDQHuFzSMcCjwKFZ/jrgQKATeAE4GiAilkk6HZiT5U6LiGU5/QXgYmBd4Pp8mZlZmzQtqUTEAmDnOvE/A2PqxAM4rodtTQYm14nPBXZc5caamVklPEyLmZlVxknFzMwq02dSyRPh6+X04ZK+L2lEX+uZmdnqp5EjlQuAFyXtBHyV4rLdnza1VWZm1i81klRW5En08cC5EfEjYKPmNsvMzPqjRq7+el7Sl4HPAH8jaQ1gzeY2y8ystfx4iWo0cqTyKUDA5/KGw+HAWU1tlZmZ9UuNHKk8SzEyMJI2iIjHgJ80tVVmZtYv9ZhUJK0FnEdxx/sjFEc1wyVdDhwXEa+2pIVmZtZv9Nb99XVgA2B4ROwUETsC7wPWB77RisaZmVn/0ltS+QfgsxHxTC0QEcuBz+cyMzOzt+j1nEpEPFcn9mwrnoFi1m6+Gshs5fWWVF7PoetVZ5mTipmZvU1vSWVTYD5OKmZm1qAek0pEDO9pmZmZWT29XVK8U28rRsQ91TfHzMz6s966v37cy7IAPlpxW8zMrJ/rrftr71Y2xMzM+r+mP6RL0gBJd0r6Zc5vJelWSZ2SLss795G0ds535vKRpW2cnPEHJe1fio/LWKekSc3eFzMz610rnvx4AnB/af57wNkRsQ3wFHBMxo8Bnsr42VkOSdsDhwE7AOOA8zJRDaDoojsA2B44PMuamVmbNDWpSBoOHARcmPMC9gWuzCJTgINzenzOk8vHZPnxwLSIeDkiHgY6gT3z1RkRCyLiFWBaljUzszZp5HHC3+o2P0DS1Aa3/0PgK8DrOb8p8HRErMj5RcCwnB4GLATI5cuz/Bvxbuv0FK+3DxMlzZU0t6urq8Gmm5nZymrkSGVUPqSrNnLxFcBjfa0k6ePA0oi4fdWauOoi4oKI6IiIjiFDhrS7OWZm71mNPE9lAnBpJpYxwKyIOLOB9T4MfELSgcA6FI8g/hEwSNLAPBoZTvHMe/J9BLBI0kBgY+DPpXhNeZ2e4mZm1gY9HqlI2ilvgNwROBM4AvgTMKOvGyMBIuLkiBgeESMpTrTfFBFHADcDh2SxCcA1OT0958nlN0VEZPywvDpsK2AUcBswh+Ioaqs8gjosy5qZWZuszM2PzwI7ZXxVbn78KjBN0neAO4GLMn4R8FNJncAyiiRBRMzPB4PdB6ygeEDYawCSjgdmAAOAyREx/x22yczMKtCSmx8j4hbglpxeQHHlVvcyLwGf7GH97wLfrRO/DriuqnaamdmqaeTqr+MlbZTT/0fSbZLGNL9pZmbW3zRy9dfEiHhG0lhgc+BY4PvNbZaZmfVHjSSV2rNTDgSmRsTdDa5nZmarmUaSw92SrgM+DlwvaQP8kC4zM6ujkftUjgZ2pxgS5QVJm/HmeF1mZmZv6DOpRMRrkh4Etpa0XQvaZGZm/VSfSUXSPwNfohhXax6wBzAb2KepLTMzs36nkXMqJwEdwCN578ruFMOnmJmZvUUjSeWliHgRigEl8671bZvbLDMz64967P4qDfq4RNIg4P9SjPu1jGKYeTMzs7fo7ZzKbcBuEfGJnP9m3km/MXBt01tmZmb9Tm9JRd0DETGriW0xM7N+rrekMkTS/+ppYUSc1YT2mJlZP9ZbUhkAbECdIxYzM7N6eksqSyLitJa1xMzM+r3eLin2EYqZma2U3pKKn5liZmYrpcekEhHLWtkQMzPr/5r2XBRJ6+RTIu+WNF/SqRnfStKtkjolXSZprYyvnfOduXxkaVsnZ/xBSfuX4uMy1ilpUrP2xczMGtPMh229DOwbETsDuwDjJI0GvgecHRHbAE/x5jD6xwBPZfzsLIek7YHDgB2AccB5kgZIGgD8GDgA2B44PMuamVmbNC2pROG5nF0zXwHsC1yZ8SnAwTk9PufJ5WMkKePTIuLliHgY6AT2zFdnRCyIiFeAaVnWzMzapKmPBc4jiruApcBM4E/A0zmmGBRjiA3L6WHAQoBcvhzYtBzvtk5PcTMza5OmJpWIeC0idgGGUxxZtOUhX5ImSporaW5XV1c7mmBmtlpoalKpiYingZuBDwKDJNVuuhwOLM7pxcAIKEZIphi48s/leLd1eorXq/+CiOiIiI4hQ4ZUsk9mZvZ2zbz6a0gOmY+kdYH9gPspksshWWwCcE1OT895cvlNEREZPyyvDtsKGEUxgvIcYFReTbYWxcn86c3aHzMz61ufjxNeBZsDU/IqrTWAyyPil5LuA6ZJ+g5wJ3BRlr8I+KmkTmAZRZIgIuZLuhy4D1gBHBcRrwFIOh6YQTFO2eR8gJiZmbVJ05JKRNwD7FonvoDi/Er3+EvAJ3vY1neB79aJXwdct8qNNTOzSrTknIqZma0enFTMzKwyTipmZlYZJxUzM6uMk4qZmVXGScXMzCrjpGJmZpVxUjEzs8o4qZiZWWWcVMzMrDJOKmZmVhknFTMzq4yTipmZVcZJxczMKuOkYmZmlXFSMTOzyjipmJlZZZxUzMysMk1LKpJGSLpZ0n2S5ks6IeObSJop6aF8H5xxSTpHUqekeyTtVtrWhCz/kKQJpfjukublOudIUrP2x8zM+tbMI5UVwJciYntgNHCcpO2BScCsiBgFzMp5gAOAUfmaCJwPRRICTgH2oni2/Sm1RJRlji2tN66J+2NmZn1oWlKJiCURcUdOPwvcDwwDxgNTstgU4OCcHg9MjcJsYJCkzYH9gZkRsSwingJmAuNy2UYRMTsiApha2paZmbVBS86pSBoJ7ArcCgyNiCW56AlgaE4PAxaWVluUsd7ii+rEzcysTZqeVCRtAFwFnBgRz5SX5RFGtKANEyXNlTS3q6ur2dWZma22mppUJK1JkVAuiYhfZPjJ7Loi35dmfDEworT68Iz1Fh9eJ/42EXFBRHRERMeQIUNWbafMzKxHzbz6S8BFwP0RcVZp0XSgdgXXBOCaUvzIvApsNLA8u8lmAGMlDc4T9GOBGbnsGUmjs64jS9syM7M2GNjEbX8Y+AwwT9JdGfsacAZwuaRjgEeBQ3PZdcCBQCfwAnA0QEQsk3Q6MCfLnRYRy3L6C8DFwLrA9fkyM7M2aVpSiYjfAj3dNzKmTvkAjuthW5OByXXic4EdV6GZZmZWId9Rb2ZmlXFSMTOzyjipmJlZZZxUzMysMk4qZmZWGScVMzOrjJOKmZlVxknFzMwq46RiZmaVcVIxM7PKOKmYmVllnFTMzKwyTipmZlYZJxUzM6uMk4qZmVXGScXMzCrjpGJmZpVxUjEzs8o4qZiZWWWallQkTZa0VNK9pdgmkmZKeijfB2dcks6R1CnpHkm7ldaZkOUfkjShFN9d0rxc5xxJata+mJlZY5p5pHIxMK5bbBIwKyJGAbNyHuAAYFS+JgLnQ5GEgFOAvYA9gVNqiSjLHFtar3tdZmbWYk1LKhHxa2BZt/B4YEpOTwEOLsWnRmE2MEjS5sD+wMyIWBYRTwEzgXG5bKOImB0RAUwtbcvMzNqk1edUhkbEkpx+Ahia08OAhaVyizLWW3xRnXhdkiZKmitpbldX16rtgZmZ9ahtJ+rzCCNaVNcFEdERER1DhgxpRZVmZqulVieVJ7PrinxfmvHFwIhSueEZ6y0+vE7czMzaqNVJZTpQu4JrAnBNKX5kXgU2Glie3WQzgLGSBucJ+rHAjFz2jKTRedXXkaVtmZlZmwxs1oYlXQrsA2wmaRHFVVxnAJdLOgZ4FDg0i18HHAh0Ai8ARwNExDJJpwNzstxpEVE7+f8FiivM1gWuz5eZmbVR05JKRBzew6IxdcoGcFwP25kMTK4TnwvsuCptNDOzavmOejMzq4yTipmZVcZJxczMKuOkYmZmlXFSMTOzyjipmJlZZZxUzMysMk4qZmZWGScVMzOrjJOKmZlVxknFzMwq46RiZmaVcVIxM7PKOKmYmVllnFTMzKwyTipmZlYZJxUzM6uMk4qZmVWm3ycVSeMkPSipU9KkdrfHzGx11q+TiqQBwI+BA4DtgcMlbd/eVpmZrb76dVIB9gQ6I2JBRLwCTAPGt7lNZmarLUVEu9vwjkk6BBgXEZ/N+c8Ae0XE8d3KTQQm5uy2wIMtauJmwP9rUV2ue/Wuu931u+73dt3vi4ghjRQc2OyWvBtExAXABa2uV9LciOhodb2ue/Wru931u+7Vq+7e9Pfur8XAiNL88IyZmVkb9PekMgcYJWkrSWsBhwHT29wmM7PVVr/u/oqIFZKOB2YAA4DJETG/zc0qa3mXm+tebetud/2ue/Wqu0f9+kS9mZm9u/T37i8zM3sXcVIxM7PKOKk0gaTJkpZKurdd9UnaRNJMSQ/l++AWtGOEpJsl3SdpvqQTml1nqe51JN0m6e6s+9RW1V1qwwBJd0r6ZYvrfUTSPEl3SZrb4roHSbpS0gOS7pf0wRbVu23ub+31jKQTW1F31n9S/p3dK+lSSeu0sO4Tst75rdznRjmpNMfFwLg21zcJmBURo4BZOd9sK4AvRcT2wGjguBYOm/MysG9E7AzsAoyTNLpFddecANzf4jprPhYRu7ThvoUfATdExHbAzrRo/yPiwdzfXYDdgReAq1tRt6RhwL8CHRGxI8VFQoe1qO4dgWMpRhPZGfi4pG1aUXejnFSaICJ+DSxrc33jgSk5PQU4uAXtWBIRd+T0sxRfMMOaXW/WFxHxXM6uma+WXYUiaThwEHBhq+psN0kbAx8FLgKIiFci4uk2NGUM8KeIeLSFdQ4E1pU0EFgPeLxF9f4VcGtEvBARK4D/Af6hRXU3xEnlvWtoRCzJ6SeAoa2sXNJIYFfg1hbWOUDSXcBSYGZEtKxu4IfAV4DXW1hnTQA3Sro9hyRqla2ALuAn2e13oaT1W1h/zWHApa2qLCIWAz8AHgOWAMsj4sYWVX8vsLekTSWtBxzIW28AbzsnldVAFNeNt/JX+wbAVcCJEfFMq+qNiNeyO2Q4sGd2FTSdpI8DSyPi9lbUV8dHImI3itG6j5P00RbVOxDYDTg/InYFnqc13axvyJuePwFc0cI6B1P0BGwFbAGsL+nTrag7Iu4HvgfcCNwA3AW81oq6G+Wk8t71pKTNAfJ9aSsqlbQmRUK5JCJ+0Yo6u8sumJtp3XmtDwOfkPQIxUjZ+0r6WYvqrv1yJiKWUpxX2LNFVS8CFpWOCK+kSDKtdABwR0Q82cI6/xZ4OCK6IuJV4BfAh1pVeURcFBG7R8RHgaeAP7aq7kY4qbx3TQcm5PQE4JpmVyhJFP3r90fEWc2ur1vdQyQNyul1gf2AB1pRd0ScHBHDI2IkRVfMTRHRkl+uktaXtGFtGhhL0UXSdBHxBLBQ0rYZGgPc14q6Sw6nhV1f6TFgtKT18m9+DC28QEPSX+T7lhTnU37eqrob0a+HaXm3knQpsA+wmaRFwCkRcVEr6wPOAC6XdAzwKHBos+ov+TDwGWBentsA+FpEXNeCujcHpuSD29YALo+Ill7a2yZDgauL7zYGAj+PiBtaWP8XgUuyG2oBcHSrKs4kuh/wuVbVCRARt0q6EriD4orHO2ntkClXSdoUeBU4rk0XR/TIw7SYmVll3P1lZmaVcVIxM7PKOKmYmVllnFTMzKwyTipmZlYZJ5XVnKTXuo32OrLdbWqFHFn2HkkndYsf/E4HwZR0lKRzGy0j6duSFufn/pCkX7RwAM7KSRpZGylb0j6NjtacIx1/4R3U13AdPaz/tXe6rvXMScVerI32mq9HygtzwLz3FEl/CewRETtFxNndFh8MtPKL/ez83EcBlwE3SRpS1cZVeLf/Px8ErHRSqYCTShO82//YrA3y1/R0STdRDJuPpC9LmpO/7k8tlf26pD9K+m3++v+3jN8iqSOnN8shTGqDPp5Z2tbnMr5PrlN7NsclebcykvaQ9HsVz0q5TdKGkn4taZdSO34raedu+7GOpJ+oeNbInZI+lotuBIblEcLepfIfohhH6sxctrWkY7Otd0u6KgfxQ9InVTzT4m5Jv67zGR4k6Q+SNmv0c4+Iy7Jt/1Rnez21Y6ikqzN+t6QP5RHDg5KmUtxdP0LS4fk53Cvpe6V/i4szNq921CbpX1U8E+ceSdPqtGWkpN9IuiNfDQ9RImmH/De8K7c/iuJG3a0zdmb3IxBJ50o6KqfH5d/HHZRG51UxssDk3PadksZn/CgVR4A3qDga/H7Gz6AYZfiu/FtbX9K1+RneK+lTje6TdRMRfq3GL4rB6O7K19UZO4piXKdNcn4sxR3Dovgh8kuKIc93B+ZRDP29EdAJ/FuucwvF8yYANgMeyemJwDdyem1gLsW1SdxnAAAFEklEQVTAfPsAyykGg1wD+APwEaB2p/Yeuc5GFHeOTwB+mLEPAHPr7NuXgMk5vR3F8BrrACOBe3v4PC4GDinNb1qa/g7wxZyeBwzL6UGlz+1c4O+B3wCD62z/KODcnP527fMqLT+RYoDG7uv11I7LKAbuhOK5Hhvn/r0OjM74FrnvQ/Kzu4niiGx3itGc6bYfjwNrl2Pd2rIesE5Oj6p99uXPNf89f1ln3f8AjsjptYB1u/97dF83P9Oj8t9uYdYp4PJaOeB/A5+utZliPKz1c70F+bmsQzG6xIgs91ypjn8E/qs0v3G7/2/215ePVKzc/fX3pfjMiKg9o2Vsvu6kGJpiO4r/2HtTJKIXohiNeHoD9Y0FjlQxjMutwKa5LYDbImJRRLxOkeRGAtsCSyJiDkBEPBPFcySuoHhA0ZrAP1Mkg+4+Avws13uA4gvlAw20sWzH/FU+DzgC2CHjvwMulnQsxZd5zb7AV4GDIuKplawLii/LlWnHvsD58MYozcsz/mhEzM7pPYBbohgAcQVwCcWPggXA+yX9h6RxQG1E6Xsohl75NMUwJN2tCfxXtuUKVq678A/A1yR9FXhfRLy4EutuRzGQ40NRfPOXB+0cC0zKv6tbKBLIlrlsVkQsj4iXKMYme1+dbc8D9pP0PUl7lz5HW0lOKtaT50vTAv69lHy2ib7HMlvBm39f5UetiuJXdm1bW8Wbz6J4uVTuNXoZmy4iXgBmUgxBfijFF2UzXAwcHxF/DZxK7ktEfB74BsWzLG5XMRYTwJ+ADVn55FWzK/UHJ6zbjl4838dyMuntTPEl/HnefMDYQcCPKUYcnqO3n1c7CXgy1+2gOOJoSET8nKKL8UXgOkn71ilW/tuBvvcVir+rfyz9XW0ZxTDx0MDfVUT8kWJ/5wHfkfStBuq0OpxUrBEzgH9W8ZwUJA1TMVLqr4GDJa2rYqTcvyut8whF9wrAId229S95hIGkD6j3Bzs9CGwuaY8sv2HpS+5C4BxgTg9HBb+h+FWPpA9Q/HJ9sI99fZYiKdRsCCzJ9h5RC0raOiJujYhvUTyoqvagpEcpulKmStqBlSDpHyl+cdcbdbduOyjOef1Lrj9AxdMYu7sN+BsV57YGUIzs+z95vmeNiLiKIkHupuKk/oiIuJniiGtjYINu29uY4ujxdYoBRAfQIEnvBxZExDkUI2fvxNs/80eB7SWtrWLk6TEZfwAYKWnrnD+8tM4M4IvSG+fhdm2gOa+W/g63AF6IiJ8BZ9L6IfzfM5xUrE95JPFz4A/Z5XElsGEUjw6+DLgbuB6YU1rtBxTJ406Kcyo1F1J0Qdyh4vLT/6T3I5JXgE8B/yHpboqjk9rRwu0UXTY/6WH184A1ss2XAUdFxMs9lK2ZBnw5T/ZuDXyTopvud7x1KP0zaye+gd/nZ1Br8wMUX/xXlL4Ae3JSnix+CPg0sG9EdNUp11M7TgA+lvt4O3W6oqJ4AugkimfM3A3cHhHXUDzq+ZbsMvoZcDJFgvhZbu9O4Jx4+yi45wET8t9jOxo4Kio5FLg369wRmBoRfwZ+lyfIz4yIhRTnS+7N9ztzP16iOCd3bZ6oLz8j6HSKbrl7JM3P+b5ckOUvAf4auC3bdQrFeSt7BzxKsVVG0rcpTn7+oEX1bUHRdbNd/mo2szbzkYr1S5KOpPjl/nUnFLN3Dx+pmJlZZXykYmZmlXFSMTOzyjipmJlZZZxUzMysMk4qZmZWmf8PXlloWfZx9OcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118e03ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task IDs mapping: \n",
      "Task ID -> 1zsCldT4p8.set1(1) is attempted 76458 times. Max seq len: 240\n",
      "Task ID -> kvig7fcCVc.partb(2) is attempted 58768 times. Max seq len: 61\n",
      "Task ID -> p7cfRPp-kQ.partb(3) is attempted 76279 times. Max seq len: 147\n",
      "Task ID -> dTZlXODVzW.partb(4) is attempted 43769 times. Max seq len: 137\n",
      "Task ID -> 1zsCldT4p8.set2(5) is attempted 75166 times. Max seq len: 210\n",
      "Task ID -> UlJYoQjOUR.partb(6) is attempted 63034 times. Max seq len: 193\n",
      "Task ID -> 9wRCzK1G7F.partb(7) is attempted 68238 times. Max seq len: 180\n",
      "Task ID -> 2yCqg9SbT0.set1(8) is attempted 46153 times. Max seq len: 119\n",
      "Task ID -> DebcfZEEmI.proper_fractions(9) is attempted 66271 times. Max seq len: 132\n",
      "Task ID -> nl-M69Ez9k.parta(10) is attempted 51483 times. Max seq len: 145\n"
     ]
    }
   ],
   "source": [
    "#frequency distribution of taskids\n",
    "cnt2 = Counter()\n",
    "another_2 = {}\n",
    "seqlen_tasks = {}\n",
    "temp_seqlen = {}\n",
    "position_2 = 1\n",
    "for i in student_vectors:\n",
    "    for k in temp_seqlen:\n",
    "        temp_seqlen[k] = 0\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['second_try'] == False):\n",
    "            if j['task_id'] not in another_2:\n",
    "                another_2[j['task_id']] = str(position_2)\n",
    "                position_2 = position_2 + 1\n",
    "            if j['task_id'] not in seqlen_tasks:\n",
    "                seqlen_tasks[j['task_id']] = []\n",
    "\n",
    "            if j['task_id'] not in temp_seqlen:\n",
    "                temp_seqlen[j['task_id']] = 1\n",
    "            else:\n",
    "                temp_seqlen[j['task_id']] += 1\n",
    "            cnt2[another_2[j['task_id']]] += 1\n",
    "    for k in seqlen_tasks:\n",
    "        seqlen_tasks[k].append(temp_seqlen[k])\n",
    "plt.bar(cnt2.keys(), cnt2.values())\n",
    "plt.title(\"Task IDs distribution\")\n",
    "plt.xlabel(\"Frequency of task ID across all students\")\n",
    "plt.ylabel(\"Task IDs\")\n",
    "plt.show()\n",
    "print(\"Task IDs mapping: \")\n",
    "for i in another_2:\n",
    "    print(\"Task ID -> \"+str(i)+\"(\"+str(another_2[i])+\") is attempted \" + str(cnt2[another_2[i]]) + \" times.\" + \" Max seq len: \"+str(max(seqlen_tasks[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 1326\n"
     ]
    }
   ],
   "source": [
    "max_seqlen = max(seqlen)\n",
    "print(\"Maximum sequence length: \"+str(max_seqlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding the sequences according to maximum sequence length. Making padded sequences of shape: number of students, maximum sequence length, length of interaction vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences have been padded according to the maximum sequence length. Final shape: (11659, 1326, 20)\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = np.zeros(shape=(len(student_vectors),max_seqlen,length_interaction_vector),dtype=float)\n",
    "for i in range(len(sequences)):\n",
    "    for j in range(len(sequences[i])):\n",
    "        padded_sequences[i][j] = sequences[i][j]\n",
    "print(\"Sequences have been padded according to the maximum sequence length. Final shape: \" + str(padded_sequences.shape))\n",
    "\n",
    "padded_output = np.zeros(shape=(len(student_vectors),max_seqlen,length_interaction_vector),dtype=float)\n",
    "for i in range(len(output_y)):\n",
    "    for j in range(len(output_y[i])):\n",
    "        padded_output[i][j] = output_y[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets. The code for random split is ready.<br>\n",
    "Review: **Try using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 11659 rows (or students) into 9327 for training and 2332 for testing.\n"
     ]
    }
   ],
   "source": [
    "#split = round((training_set_split+validation_set_split)*len(student_vectors))\n",
    "split = int((training_set_split)*len(student_vectors))\n",
    "\n",
    "tmp_rnd = 0\n",
    "training_x = np.zeros_like(padded_sequences[:split])\n",
    "training_y = np.zeros_like(np.asarray(padded_output)[:split])\n",
    "training_seqlen = np.zeros_like(seqlen[:split])\n",
    "\n",
    "test_x = np.zeros_like(padded_sequences[split:])\n",
    "test_y = np.zeros_like(np.asarray(padded_output)[split:])\n",
    "test_seqlen = np.zeros_like(seqlen[split:])\n",
    "\n",
    "itr = 0\n",
    "itr_tr = 0\n",
    "itr_te = 0\n",
    "# import random\n",
    "# #separating training and testing sets randomly\n",
    "# for i in range(len(padded_sequences)):\n",
    "#     if(random.uniform(0, 1) <= (training_set_split+validation_set_split) and itr_tr < split):\n",
    "#         #add to training\n",
    "#         training_x[itr_tr] = padded_sequences[itr]\n",
    "#         training_y[itr_tr] = np.asarray(padded_output)[itr]\n",
    "#         training_y_taskid[itr_tr] = np.asarray(padded_output_taskid)[itr]\n",
    "#         training_seqlen[itr_tr] = seqlen[itr]\n",
    "#         itr_tr = itr_tr + 1\n",
    "#     elif(itr_te < (len(student_vectors) - split)):\n",
    "#         #add to testing\n",
    "#         test_x[itr_te] = padded_sequences[itr]\n",
    "#         test_y[itr_te] = np.asarray(padded_output)[itr]\n",
    "#         test_y_taskid[itr_te] = np.asarray(padded_output_taskid)[itr]\n",
    "#         test_seqlen[itr_te] = seqlen[itr]\n",
    "#         itr_te = itr_te + 1\n",
    "#     else:\n",
    "#         #add to training\n",
    "#         training_x[itr_tr] = padded_sequences[itr]\n",
    "#         training_y[itr_tr] = np.asarray(padded_output)[itr]\n",
    "#         training_y_taskid[itr_tr] = np.asarray(padded_output_taskid)[itr]\n",
    "#         training_seqlen[itr_tr] = seqlen[itr]\n",
    "#         itr_tr = itr_tr + 1\n",
    "#     itr = itr + 1\n",
    "\n",
    "#separating training & validation set\n",
    "training_x = padded_sequences[:split]\n",
    "training_y = np.asarray(padded_output)[:split]\n",
    "training_seqlen = seqlen[:split]\n",
    "\n",
    "#separating test set\n",
    "test_x = padded_sequences[split:]\n",
    "test_y = np.asarray(padded_output)[split:]\n",
    "test_seqlen = seqlen[split:]\n",
    "\n",
    "# #generating validation and training sets by implementing k-fold cross validation (k = maximum_position)\n",
    "# validation_set_size = math.floor(validation_set_split * len(student_vectors))\n",
    "# training_set_size = len(training_x) - validation_set_size\n",
    "# maximum_position = math.floor(len(training_x) / validation_set_size)\n",
    "\n",
    "# def get_next_train_valid_set(position):\n",
    "#     if(position>=maximum_position):\n",
    "#         position = position % maximum_position\n",
    "#     print(\"Picking validation set from position: \"+str(position))\n",
    "#     valid_start = position*validation_set_size\n",
    "#     valid_end = valid_start + validation_set_size\n",
    "    \n",
    "#     valid_set_x = training_x[valid_start : valid_end]\n",
    "#     valid_set_y = training_y[valid_start : valid_end]\n",
    "#     valid_set_y_taskid = np.asarray(training_y_taskid)[valid_start : valid_end]\n",
    "#     valid_set_seqlen = np.asarray(training_seqlen[valid_start:valid_end])\n",
    "    \n",
    "#     train_set_x = np.concatenate((training_x[:valid_start], training_x[valid_end:]))\n",
    "#     train_set_y = np.concatenate((training_y[:valid_start], training_y[valid_end:]))\n",
    "#     train_set_y_taskid = np.concatenate((np.asarray(training_y_taskid)[:valid_start], np.asarray(training_y_taskid)[valid_end:]))\n",
    "#     train_set_seqlen = np.concatenate((np.asarray(training_seqlen)[:valid_start],np.asarray(training_seqlen)[valid_end:]))\n",
    "    \n",
    "#     if(len(train_set_x) != training_set_size): #test\n",
    "#         print(\"that's not good it is:\")\n",
    "#         print(train_set_x.shape)\n",
    "    \n",
    "#     return (train_set_seqlen,valid_set_seqlen,valid_set_x,valid_set_y,valid_set_y_taskid,train_set_x,train_set_y,train_set_y_taskid)\n",
    "\n",
    "# print(\"Splitting \"+str(len(student_vectors))+\" rows randomly into \"+str(training_set_size)+ \" for training, \"+str(validation_set_size)+\" for validation and \"+str(len(test_x)) + \" for testing.\")\n",
    "# print(\"Implemented \"+str(maximum_position)+\"-fold cross validation.\")\n",
    "print(\"Splitting \"+str(len(student_vectors))+\" rows (or students) into \"+str(len(training_x))+ \" for training and \"+str(len(test_x)) + \" for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1234)\n",
    "#defining placeholders\n",
    "x = tf.placeholder(tf.float32, [None, max_seqlen, length_interaction_vector])\n",
    "y = tf.placeholder(tf.float32, [None, max_seqlen, length_interaction_vector])\n",
    "seqlen_tf = tf.placeholder(tf.float32,[None])\n",
    "condition = tf.placeholder(tf.int32, shape=[], name=\"condition\")\n",
    "\n",
    "#defining tensorflow variables\n",
    "learning_tf_rate = tf.Variable(0.0, name=\"learning_tf_rate\",dtype=tf.float32,trainable=False)\n",
    "\n",
    "#dynamic RNN definition\n",
    "def dynamicRNN(x):\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32,sequence_length=seqlen_tf)\n",
    "    out_size = int(length_interaction_vector / 2)\n",
    "    outputs = tf.contrib.layers.fully_connected(outputs, out_size, activation_fn = tf.nn.sigmoid, weights_initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    opposites = tf.subtract(tf.ones(tf.shape(outputs)),outputs)\n",
    "    outputs1 = tf.concat([outputs,opposites],2)\n",
    "    return outputs1\n",
    "\n",
    "#making predictions\n",
    "pred = dynamicRNN(x)\n",
    "pred = pred*y\n",
    "# Define loss and optimizer\n",
    "cost1 = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "mask = tf.cond(condition < 1, lambda: tf.cast(tf.sequence_mask(lengths=sequences_lengths1[:split], maxlen = max_seqlen), tf.float32), lambda: tf.cast(tf.sequence_mask(lengths=sequences_lengths1[split:], maxlen = max_seqlen), tf.float32))\n",
    "cost1 = tf.multiply(cost1,tf.transpose(mask, perm=[0, 2, 1]))\n",
    "cost1 = tf.reduce_sum(cost1, 1)\n",
    "cost1 /= tf.cond(condition < 1, lambda: tf.cast(sequences_lengths1[:split],tf.float32), lambda: tf.cast(sequences_lengths1[split:],tf.float32) )\n",
    "cost = tf.reduce_mean(cost1)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_tf_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning test set into separate task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_list_dict = {}\n",
    "test_y_list_dict = {}\n",
    "test_seqlen_list_dict = {}\n",
    "train_x_list_dict = {}\n",
    "train_y_list_dict = {}\n",
    "train_seqlen_list_dict = {}\n",
    "split_dict = {}\n",
    "for i in another_2: \n",
    "    #generate encoding here! \n",
    "    sequences = []\n",
    "    sequences_lengths = []\n",
    "    for p in student_vectors:\n",
    "        interactions = []\n",
    "        interactions.append(np.concatenate([incorrect_tid_vec,incorrect_tid_vec])) #for getting the first prediction!\n",
    "        for j in student_vectors[p]:\n",
    "            if(j['task_id'] == i and j['second_try'] == False):\n",
    "                if(j['correct'] == True):\n",
    "                    vec = np.concatenate([task_ids_dict[j['task_id']],incorrect_tid_vec])\n",
    "                    interactions.append(vec)\n",
    "                else:\n",
    "                    interactions.append(np.concatenate([incorrect_tid_vec,task_ids_dict[j['task_id']]]))\n",
    "        if(len(interactions) > 1):\n",
    "            sequences_lengths.append(len(interactions)-1)\n",
    "            sequences.append(interactions)\n",
    "    split = int(0.8*len(sequences))\n",
    "    #add padding\n",
    "    padded_sequences = np.zeros([len(sequences),max_seqlen+1,length_interaction_vector])\n",
    "    for p in range(len(sequences)):\n",
    "        for j in range(len(sequences[p])):\n",
    "            padded_sequences[p][j] = sequences[p][j]\n",
    "    test_x_list_dict[i] = padded_sequences[split:,:-1]\n",
    "    test_y_list_dict[i] = padded_sequences[split:,1:]\n",
    "    test_seqlen_list_dict[i] = sequences_lengths[split:]\n",
    "    train_x_list_dict[i] = padded_sequences[:split,:-1]\n",
    "    train_y_list_dict[i] = padded_sequences[:split,1:]\n",
    "    train_seqlen_list_dict[i] = sequences_lengths[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "def calculate_auc (y_true,y_pred,sequence_lengths=[],plot=False,debug=False):\n",
    "    if sequence_lengths == []:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        con_y_true = np.zeros([sum(sequence_lengths),length_interaction_vector])\n",
    "        con_y_pred = np.zeros([sum(sequence_lengths),length_interaction_vector])\n",
    "        index = 0\n",
    "        for i in range(len(y_true)): #per student\n",
    "            for j in range(sequence_lengths[i]): #up to the sequence length\n",
    "                con_y_true[index] = y_true[i][j]\n",
    "                con_y_pred[index] = y_pred[i][j]\n",
    "                index += 1\n",
    "        con1_y_true = np.zeros([sum(sequence_lengths)])\n",
    "        con1_y_pred = np.zeros([sum(sequence_lengths)])\n",
    "        \n",
    "        for l in range(sum(sequence_lengths)):\n",
    "            index_one = np.argmax(con_y_true[l])\n",
    "            if(index_one >= int(length_interaction_vector/2)):\n",
    "                index_two = index_one - int(length_interaction_vector/2)\n",
    "            else:\n",
    "                index_two = index_one\n",
    "                index_one = index_one + int(length_interaction_vector/2)\n",
    "            if(np.argmax(con_y_true[l]) == index_one): #true is incorrect\n",
    "                con1_y_true[l] = 0.\n",
    "                con1_y_pred[l] = 1.0 - con_y_pred[l][index_one]\n",
    "            elif(np.argmax(con_y_true[l]) == index_two):\n",
    "                con1_y_true[l] = 1.\n",
    "                con1_y_pred[l] = con_y_pred[l][index_two]\n",
    "        debug=False\n",
    "        if(debug):\n",
    "            print(np.c_[con1_y_true,con1_y_pred])\n",
    "        fpr, tpr, thresholds = roc_curve(con1_y_true, con1_y_pred)\n",
    "        #print(\"tpr: \"+str(tpr) + \", fpr: \"+str(fpr) + \", thresholds: \"+str(thresholds))\n",
    "        if(plot):\n",
    "            return [roc_auc_score(con1_y_true, con1_y_pred),fpr,tpr]\n",
    "        else:\n",
    "            return roc_auc_score(con1_y_true, con1_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss = 0.68168664, Train AUC = 0.6533182445867376, Test Loss: 0.68163747, Test AUC: 0.6542869037569267\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.600374110821459, Test AUC: 0.5971027248989621\n",
      "kvig7fcCVc.partb -> Train AUC = 0.7032447640867853, Test AUC: 0.7106950992162261\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.693316707049741, Test AUC: 0.7342622757939652\n",
      "dTZlXODVzW.partb -> Train AUC = 0.6296216586198917, Test AUC: 0.6089523856715873\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.548633310137118, Test AUC: 0.5379141004153085\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.5042757831493871, Test AUC: 0.4469207230730544\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.5302084969020459, Test AUC: 0.5179692009717649\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.5754864938768369, Test AUC: 0.5561960528981489\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.7387827767898163, Test AUC: 0.7232035475303122\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.4926417800054649, Test AUC: 0.4957860813705136\n",
      "\n",
      "\n",
      "\n",
      "Epoch 20, Train loss = 0.67711, Train AUC = 0.6781088325589765, Test Loss: 0.6770706, Test AUC: 0.6775597958912725\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.5755072977374387, Test AUC: 0.5682897015606849\n",
      "kvig7fcCVc.partb -> Train AUC = 0.6112524594926513, Test AUC: 0.6202492362351517\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.5675553278732548, Test AUC: 0.5758021092205143\n",
      "dTZlXODVzW.partb -> Train AUC = 0.6087576139856992, Test AUC: 0.5818646160754378\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.5006096046591462, Test AUC: 0.48505211905438056\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.6218457601264493, Test AUC: 0.5767057765643958\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.46744127304991706, Test AUC: 0.4596024466738579\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.5468120037329189, Test AUC: 0.5242679321632687\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.5846369884111455, Test AUC: 0.5890400200425276\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.5625036558621515, Test AUC: 0.557444689053958\n",
      "\n",
      "\n",
      "\n",
      "Epoch 40, Train loss = 0.6770329, Train AUC = 0.7075160046694394, Test Loss: 0.67700374, Test AUC: 0.6959298011521406\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.6072917023912965, Test AUC: 0.6031039592213134\n",
      "kvig7fcCVc.partb -> Train AUC = 0.6014008929003746, Test AUC: 0.6095990356553738\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.5612716186106088, Test AUC: 0.5700946874691072\n",
      "dTZlXODVzW.partb -> Train AUC = 0.612932279021629, Test AUC: 0.5980711903976762\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.5007260707335055, Test AUC: 0.48504372919926586\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.6264946676222324, Test AUC: 0.5839235447352166\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.4644831482452909, Test AUC: 0.45776378223040604\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.5529296079410283, Test AUC: 0.5337005446024908\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.5971925874848708, Test AUC: 0.59594511105253\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.5696320405395526, Test AUC: 0.5636050513306947\n",
      "\n",
      "\n",
      "\n",
      "Epoch 60, Train loss = 0.6768819, Train AUC = 0.6792269043581237, Test Loss: 0.67689556, Test AUC: 0.668063065260242\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.6391474280624871, Test AUC: 0.6363210151592605\n",
      "kvig7fcCVc.partb -> Train AUC = 0.6109829304607339, Test AUC: 0.6239837291245742\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.5610565372220119, Test AUC: 0.5696744958539897\n",
      "dTZlXODVzW.partb -> Train AUC = 0.6008766156833826, Test AUC: 0.5914716465068852\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.5078718191853359, Test AUC: 0.4887796838914994\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.6328273514495322, Test AUC: 0.592556454605602\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.4684185647275653, Test AUC: 0.46236007415237207\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.5534266323385774, Test AUC: 0.5347841516143478\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.6754874700332493, Test AUC: 0.6639792127501287\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.5721152131959063, Test AUC: 0.5658300752718881\n",
      "\n",
      "\n",
      "\n",
      "Epoch 80, Train loss = 0.6763003, Train AUC = 0.7342656771016403, Test Loss: 0.6763981, Test AUC: 0.7308532759692\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.6352043757605099, Test AUC: 0.6294653249767205\n",
      "kvig7fcCVc.partb -> Train AUC = 0.638070924174997, Test AUC: 0.6534617824758671\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.5964903978981779, Test AUC: 0.6101029404289621\n",
      "dTZlXODVzW.partb -> Train AUC = 0.61210948393785, Test AUC: 0.6067961938472909\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.515885998119203, Test AUC: 0.49215575096955927\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.6414433827412341, Test AUC: 0.5984132499102781\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.4684896190393146, Test AUC: 0.4625904302222438\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.5542596985511478, Test AUC: 0.5354452451910774\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.7351258543265191, Test AUC: 0.7201741255218045\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.5796944043355647, Test AUC: 0.5711383578654095\n",
      "\n",
      "\n",
      "\n",
      "Epoch 100, Train loss = 0.67604345, Train AUC = 0.8610737378238233, Test Loss: 0.6761894, Test AUC: 0.8427807299214696\n",
      "1zsCldT4p8.set1 -> Train AUC = 0.6289166210723792, Test AUC: 0.6245624035521604\n",
      "kvig7fcCVc.partb -> Train AUC = 0.6564077184051482, Test AUC: 0.6704487911530165\n",
      "p7cfRPp-kQ.partb -> Train AUC = 0.6062274916439526, Test AUC: 0.6216466179432918\n",
      "dTZlXODVzW.partb -> Train AUC = 0.6193964970353959, Test AUC: 0.617139081151306\n",
      "1zsCldT4p8.set2 -> Train AUC = 0.5284124906897998, Test AUC: 0.5015467069606274\n",
      "UlJYoQjOUR.partb -> Train AUC = 0.641236812070215, Test AUC: 0.5981444656847307\n",
      "9wRCzK1G7F.partb -> Train AUC = 0.48541458131657245, Test AUC: 0.47871445265697904\n",
      "2yCqg9SbT0.set1 -> Train AUC = 0.6438427312799883, Test AUC: 0.6608558629403736\n",
      "DebcfZEEmI.proper_fractions -> Train AUC = 0.7341121571456878, Test AUC: 0.7198420985764095\n",
      "nl-M69Ez9k.parta -> Train AUC = 0.5811787391612971, Test AUC: 0.5723656704324992\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_all_tasks = {}\n",
    "true_all_tasks['train'] = {}\n",
    "true_all_tasks['test'] = {}\n",
    "predictions_all_tasks = {}\n",
    "predictions_all_tasks['train'] = {}\n",
    "predictions_all_tasks['test'] = {}\n",
    "seqlen_all_tasks = {}\n",
    "seqlen_all_tasks['train'] = {}\n",
    "seqlen_all_tasks['test'] = {}\n",
    "for i in another_2:\n",
    "    seqlen_all_tasks['train'][i] = []\n",
    "    seqlen_all_tasks['test'][i] = []\n",
    "    predictions_all_tasks['train'][i] = []\n",
    "    predictions_all_tasks['test'][i] = []\n",
    "    true_all_tasks['train'][i] = []\n",
    "    true_all_tasks['test'][i] = []\n",
    "seqlen_all_tasks['train']['overall'] = []\n",
    "seqlen_all_tasks['test']['overall'] = []\n",
    "predictions_all_tasks['train']['overall'] = []\n",
    "predictions_all_tasks['test']['overall'] = []\n",
    "true_all_tasks['train']['overall'] = []\n",
    "true_all_tasks['test']['overall'] = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for l_r in learning_rate:\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        assign_op = learning_tf_rate.assign(l_r)\n",
    "        sess.run(assign_op)\n",
    "        cost_prev = 1.0\n",
    "        stop = False\n",
    "        #print(\"Final Learning Rate: \"+str(learning_tf_rate.eval()))\n",
    "        step = 0\n",
    "        while(stop==False):\n",
    "        #for step in range(1, training_steps):\n",
    "            sess.run(optimizer, feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "            step += 1\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                loss = sess.run(cost, feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "                cost_current = loss\n",
    "                if cost_prev - cost_current <= 0.00005:\n",
    "                    stop = True\n",
    "                else:\n",
    "                    cost_prev = cost_current\n",
    "    \n",
    "                #calculate overall training AUC\n",
    "                pred_train,train_cost = sess.run([pred,cost], feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "                o_train_opts_taskid = calculate_auc(training_y,pred_train,training_seqlen)\n",
    "                true_all_tasks['train']['overall'].append(training_y)\n",
    "                predictions_all_tasks['train']['overall'].append(pred_train)\n",
    "                seqlen_all_tasks['train']['overall'].append(o_train_opts_taskid)\n",
    "                # Calculate overall test auc\n",
    "                pred_test,test_cost = sess.run([pred,cost],feed_dict={x: test_x, y: test_y, seqlen_tf: test_seqlen,condition:1})\n",
    "                o_temp_opts_taskid = calculate_auc(test_y,pred_test,test_seqlen)\n",
    "                true_all_tasks['test']['overall'].append(test_y)\n",
    "                predictions_all_tasks['test']['overall'].append(pred_test)\n",
    "                seqlen_all_tasks['test']['overall'].append(o_temp_opts_taskid)\n",
    "                #print status\n",
    "                print(\"Epoch \" + str(step) + \", Train loss = \" + str(train_cost) + \", Train AUC = \"+str(o_train_opts_taskid) +\", Test Loss: \"+str(test_cost)+ \", Test AUC: \"+str(o_temp_opts_taskid))\n",
    "                \n",
    "                #calculate train and test AUCs on task IDs separately!\n",
    "                for idx,i in enumerate(another_2):\n",
    "                    pred_train = sess.run(pred, feed_dict={x: train_x_list_dict[i], y: train_y_list_dict[i], seqlen_tf: train_seqlen_list_dict[i] ,condition:0})\n",
    "                    if(i=='9wRCzK1G7F.partb' or i=='1zsCldT4p8.set2'):\n",
    "                        train_opts_taskid = calculate_auc(train_y_list_dict[i],pred_train,train_seqlen_list_dict[i],debug=True)\n",
    "                    else:\n",
    "                        train_opts_taskid = calculate_auc(train_y_list_dict[i],pred_train,train_seqlen_list_dict[i])\n",
    "                    true_all_tasks['train'][i].append(train_y_list_dict[i])\n",
    "                    predictions_all_tasks['train'][i].append(pred_train)\n",
    "                    seqlen_all_tasks['train'][i].append(train_opts_taskid)\n",
    "                    pred_test = sess.run(pred,feed_dict={x: test_x_list_dict[i], y: test_y_list_dict[i], seqlen_tf: test_seqlen_list_dict[i] ,condition:1})\n",
    "                    if(i=='9wRCzK1G7F.partb' or i=='1zsCldT4p8.set2'):\n",
    "                        temp_opts_taskid = calculate_auc(test_y_list_dict[i],pred_test,test_seqlen_list_dict[i],debug=True)\n",
    "                    else:\n",
    "                        temp_opts_taskid = calculate_auc(test_y_list_dict[i],pred_test,test_seqlen_list_dict[i])\n",
    "                    true_all_tasks['test'][i].append(test_y_list_dict[i])\n",
    "                    predictions_all_tasks['test'][i].append(pred_test)\n",
    "                    seqlen_all_tasks['test'][i].append(temp_opts_taskid)\n",
    "                    print(str(i) + \" -> Train AUC = \"+str(train_opts_taskid) + \", Test AUC: \"+str(temp_opts_taskid))\n",
    "                \n",
    "#                 view_point = np.argsort(test_seqlen)[1]\n",
    "#                 prediction = sess.run(pred, feed_dict={x: [test_x[view_point]], y: [test_y[view_point]], seqlen_tf: [test_seqlen[view_point]],condition:1})\n",
    "#                 print(\"\\n\\ntrue:prediction\\n\")\n",
    "#                 print(np.c_[test_x[view_point][:test_seqlen[view_point]],prediction[0][:test_seqlen[view_point]]])\n",
    "                print(\"\\n\\n\")\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "        for idx,i in enumerate(another_2):\n",
    "            pred_test = sess.run(pred,feed_dict={x: test_x_list_dict[i], y: test_y_list_dict[i], seqlen_tf: test_seqlen_list_dict[i] ,condition:1})\n",
    "            true_all_tasks['test'][i].append(test_y_list_dict[i])\n",
    "            predictions_all_tasks['test'][i].append(pred_test)\n",
    "            seqlen_all_tasks['test'][i].append(test_seqlen_list_dict[i])\n",
    "            pred_train = sess.run(pred, feed_dict={x: train_x_list_dict[i], y: train_y_list_dict[i], seqlen_tf: train_seqlen_list_dict[i] ,condition:0})\n",
    "            true_all_tasks['train'][i].append(train_y_list_dict[i])\n",
    "            predictions_all_tasks['train'][i].append(pred_train)\n",
    "            seqlen_all_tasks['train'][i].append(train_seqlen_list_dict[i])\n",
    "        \n",
    "        pred_test,test_cost = sess.run([pred,cost],feed_dict={x: test_x, y: test_y, seqlen_tf: test_seqlen,condition:1})\n",
    "        true_all_tasks['test']['overall'].append(test_y)\n",
    "        predictions_all_tasks['test']['overall'].append(pred_test)\n",
    "        seqlen_all_tasks['test']['overall'].append(test_seqlen)\n",
    "        pred_train,train_cost = sess.run([pred,cost], feed_dict={x: training_x, y: training_y, seqlen_tf: training_seqlen,condition:0})\n",
    "        true_all_tasks['train']['overall'].append(training_y)\n",
    "        predictions_all_tasks['train']['overall'].append(pred_train)\n",
    "        seqlen_all_tasks['train']['overall'].append(training_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating over-all AUC\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-556940b46774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating over-all AUC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mo_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseqlen_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mo_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseqlen_all_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall AUC train: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", test: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_auc' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Calculating over-all AUC\")\n",
    "o_train = calculate_auc(true_all_tasks['train']['overall'][-1],predictions_all_tasks['train']['overall'][-1],seqlen_all_tasks['train']['overall'][-1],plot=False)\n",
    "o_test = calculate_auc(true_all_tasks['test']['overall'][-1],predictions_all_tasks['test']['overall'][-1],seqlen_all_tasks['test']['overall'][-1],plot=False)\n",
    "print(\"Overall AUC train: \"+str(o_train) + \", test: \"+str(o_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(14, 9), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Bayesian Knowledge Tracing (combined model, test set)')\n",
    "\n",
    "# for task_id in another_2:\n",
    "#     roc = calculate_auc(true_all_tasks['test'][task_id][-1] , predictions_all_tasks['test'][task_id][-1] , seqlen_all_tasks['test'][task_id][-1],plot=True)\n",
    "#     plt.plot(roc[1], roc[2], label=task_id)\n",
    "\n",
    "# roc = calculate_auc(true_all_tasks['test']['overall'][-1],predictions_all_tasks['test']['overall'][-1],seqlen_all_tasks['test']['overall'][-1],plot=True)\n",
    "# plt.plot(roc[1], roc[2], label='overall-test')\n",
    "\n",
    "# plt.legend(loc=\"lower right\")\n",
    "save_obj(true_all_tasks,\"true_all_tasks\")\n",
    "save_obj(predictions_all_tasks,\"predictions_all_tasks\")\n",
    "save_obj(seqlen_all_tasks,\"seqlen_all_tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(14, 9), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Bayesian Knowledge Tracing (combined model, Train set)')\n",
    "\n",
    "# for task_id in another_2:\n",
    "#     roc = calculate_auc(true_all_tasks['train'][task_id][-1],predictions_all_tasks['train'][task_id][-1], seqlen_all_tasks['train'][task_id][-1], plot=True)\n",
    "#     plt.plot(roc[1], roc[2], label=task_id)\n",
    "\n",
    "# roc = calculate_auc(true_all_tasks['train']['overall'][-1],predictions_all_tasks['train']['overall'][-1],seqlen_all_tasks['train']['overall'][-1],plot=True)\n",
    "# plt.plot(roc[1], roc[2], label='overall-train')\n",
    "\n",
    "# plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,2, figsize=(20, 30), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=0.1)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i,value in enumerate(another_2):\n",
    "    train_auc = seqlen_all_tasks['train'][value][:-1]\n",
    "    test_auc = seqlen_all_tasks['test'][value][:-1]\n",
    "    axs[i-1].plot(test_auc,label='test')\n",
    "    axs[i-1].plot(train_auc,label='train')\n",
    "    axs[i-1].set_ylim(0,1.1)\n",
    "    axs[i-1].set_xlim(0,12)\n",
    "    axs[i-1].set_xlabel(\"Epochs\")\n",
    "    axs[i-1].set_ylabel(\"AUC\")\n",
    "    axs[i-1].legend(loc=\"best\")\n",
    "    axs[i-1].set_title(\"Task ID = \"+str(value))\n",
    "    \n",
    "train_auc = seqlen_all_tasks['train']['overall'][:-1]\n",
    "test_auc = seqlen_all_tasks['test']['overall'][:-1]\n",
    "axs[i].plot(test_auc,label='test')\n",
    "axs[i].plot(train_auc,label='train')\n",
    "axs[i].set_ylim(0,1.1)\n",
    "axs[i].set_xlim(0,12)\n",
    "axs[i].set_xlabel(\"Epochs\")\n",
    "axs[i].set_ylabel(\"AUC\")\n",
    "axs[i].legend(loc=\"best\")\n",
    "axs[i].set_title(\"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in another_2:\n",
    "    print(i + \" final train acc: \" + str(round(seqlen_all_tasks['train'][i][-2],3)) + \"; final test acc: \"+str(round(seqlen_all_tasks['test'][i][-2],3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No longer maintained below this point.**<br>Training the model for hyperparameter (learning rate) tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_lr = []\n",
    "# plot_valid_auc_taskid = []\n",
    "# plot_train_auc_taskid = []\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     for l_r in learning_rate:\n",
    "#         plot_lr.append(l_r)    \n",
    "#         valid_taskid_list = []\n",
    "#         for k_fold in range(1,maximum_position+1):\n",
    "#             # Initialize the variables (i.e. assign their default value)\n",
    "#             tf.reset_default_graph()\n",
    "#             print(str(k_fold)+\"-fold cross-validation\")\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "#             sess.run(tf.local_variables_initializer())\n",
    "#             assign_op = learning_tf_rate.assign(l_r)\n",
    "#             sess.run(assign_op)\n",
    "#             print(\"Current Learning Rate: \"+str(learning_tf_rate.eval()))\n",
    "#             train_set_seqlen, valid_set_seqlen, valid_set_x, valid_set_y, valid_set_y_taskid, train_set_x, train_set_y, train_set_y_taskid = get_next_train_valid_set(k_fold-1)\n",
    "            \n",
    "#             for step in range(1, training_steps+1):\n",
    "#                 batch_x = train_set_x\n",
    "#                 batch_y = train_set_y\n",
    "#                 sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, y_taskid: train_set_y_taskid, seqlen_tf: train_set_seqlen})\n",
    "\n",
    "#                 if step % display_step == 0 or step == 1:\n",
    "#                     loss,trainAUC,trainOPTS = sess.run([cost, auc, opts], feed_dict={x: batch_x, y: batch_y, y_taskid: train_set_y_taskid, seqlen_tf: train_set_seqlen})\n",
    "#                     #print status\n",
    "#                     print(\"Step \" + str(step) + \", Loss = \" + str(loss) + \", Learning Rate = \"+str(learning_tf_rate.eval()) + \", Train AUC:\" + str(trainOPTS))\n",
    "#             #calculate validation AUC\n",
    "#             valid_auc_taskid, valid_opts_taskid = sess.run([auc, opts], feed_dict={x: valid_set_x, y: valid_set_y, y_taskid: valid_set_y_taskid, seqlen_tf: valid_set_seqlen})\n",
    "#             print(\"Valid_auc_taskid: \" + str(valid_opts_taskid) + \" with k = \"+str(k_fold))\n",
    "#             valid_taskid_list.append(valid_opts_taskid)\n",
    "#             print(\"Optimization Finished!\")\n",
    "    \n",
    "#         #calculate training AUC (it should take both validation and training sets)\n",
    "#         train_auc_taskid, train_opts_taskid = sess.run([auc, opts], feed_dict={x: training_x, y: training_y, y_taskid: training_y_taskid, seqlen_tf: training_seqlen})\n",
    "\n",
    "#         print(\"Train_auc_taskid: \" + str(train_opts_taskid))\n",
    "#         plot_train_auc_taskid.append(train_opts_taskid)\n",
    "        \n",
    "#         #take average of validation AUCs\n",
    "#         valid_avg_taskid = np.mean(valid_taskid_list)\n",
    "        \n",
    "#         print(\"Average Valid_auc_taskid: \" + str(valid_avg_taskid))\n",
    "#         plot_valid_auc_taskid.append(valid_avg_taskid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting validation set ccssm auc across different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title(\"Training Set AUC\")\n",
    "# plt.xlabel(\"Learning Rate Index\")\n",
    "# plt.ylabel(\"AUC\")\n",
    "# plt.plot(plot_train_auc_taskid,'r--',label='train')\n",
    "# plt.plot(plot_valid_auc_taskid,label='valid')\n",
    "# plt.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "# plt.show()\n",
    "# print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
