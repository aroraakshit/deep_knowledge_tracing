{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can work with either tf.VERSION = '1.4.1' (for MacOS High Sierra) or tf.VERSION = '0.12.1' (for RedHat based SuperComputer), functions may change for other versions. Also, this model does not have mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_split = 0.8\n",
    "validation_set_split = 0.1\n",
    "learning_rate = 0.1\n",
    "num_units = 5 #number of units in RNN cell\n",
    "training_steps = 20 #number of epochs\n",
    "display_step = 1 #number of epochs after which to display progress\n",
    "optimize_using = \"adagrad\" #other option: \"momentum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading JSON file into dictionary called 'student_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"student_vectors_n_task_10_n_limit_10000.json\"\n",
    "student_vectors = json.load(open(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting unique CCSSM labels and Task IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique CCSSM Labels: 4\n",
      "Number of unique task IDs: 10\n",
      "Number of students: 1255\n"
     ]
    }
   ],
   "source": [
    "ccssm_labels = []\n",
    "task_ids = []\n",
    "for i in student_vectors:\n",
    "    for j in student_vectors[i]:\n",
    "        if j['ccssm'] not in ccssm_labels:\n",
    "            ccssm_labels.append(j['ccssm'])\n",
    "        if j['task_id'] not in task_ids:\n",
    "            task_ids.append(j['task_id'])\n",
    "print(\"Number of unique CCSSM Labels: \" + str(len(ccssm_labels)))\n",
    "print(\"Number of unique task IDs: \" + str(len(task_ids)))\n",
    "print(\"Number of students: \" + str(len(student_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 1-hot encoding for Task IDs and CCSSM Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for using MultiLabelBinarizer\n",
    "temp_ids = []\n",
    "for i in task_ids:\n",
    "    temp_ids.append([i])\n",
    "temp_labels = []\n",
    "for i in ccssm_labels:\n",
    "    temp_labels.append([i])\n",
    "    \n",
    "#generating encodings\n",
    "enc = MultiLabelBinarizer()\n",
    "task_ids_1hot = (enc.fit_transform(temp_ids)).astype(float)\n",
    "task_ids_classes = enc.classes_\n",
    "task_ids_dict = dict(zip(task_ids, task_ids_1hot))\n",
    "labels_1hot = enc.fit_transform(temp_labels).astype(float)\n",
    "labels_classes = enc.classes_\n",
    "labels_dict = dict(zip(ccssm_labels,labels_1hot))\n",
    "#pp.pprint(labels_dict)\n",
    "#pp.pprint(task_ids_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating input sequences of interactions to feed the network. Say we have 3 task IDs and 3 labels; here is an example of interaction vectors generated:\n",
    "1. User correctly solves task 2 of label 3: [010   000   001 000]\n",
    "2. User incorrectly solves task 1 of label 2: [000   100   000   010]\n",
    "\n",
    "1-hot representation of task IDs: \n",
    "task ID 1: 1,0,0 ; \n",
    "task ID 2: 0,1,0 ; \n",
    "task ID 3: 0,0,1 ; \n",
    "and similarly for labels!\n",
    "\n",
    "In the interaction vector, first 3 bits belong to taskID that user solved correctly; next 3 bits belong to taskID that user solved incorrectly; next 3 bits belong to label corresponding to task ID solved by user correctly and last 3 bits belong to label corresponding to the task ID solved by the user incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "output_y_ccssm = []\n",
    "output_y_taskid = []\n",
    "output_y = []\n",
    "seqlen = []\n",
    "incorrect_tid_vec = np.zeros((len(task_ids)), dtype=np.float)\n",
    "incorrect_csm_vec = np.zeros((len(ccssm_labels)),dtype=np.float)\n",
    "for i in student_vectors:\n",
    "    temp_seq = []\n",
    "    for j in student_vectors[i]:\n",
    "        if(j['second_try'] == False): #ignoring second_try\n",
    "            if(j['correct'] == True):\n",
    "                vec = np.concatenate([task_ids_dict[j['task_id']],incorrect_tid_vec,labels_dict[j['ccssm']],incorrect_csm_vec])\n",
    "                temp_seq.append(vec)\n",
    "            else:\n",
    "                vec = np.concatenate([incorrect_tid_vec,task_ids_dict[j['task_id']],incorrect_csm_vec,labels_dict[j['ccssm']]])\n",
    "                temp_seq.append(vec)\n",
    "    seqlen.append(len(temp_seq))\n",
    "    last_one = temp_seq.pop()\n",
    "    output_y.append(last_one)\n",
    "    output_y_ccssm.append(last_one[2*len(task_ids):])\n",
    "    output_y_taskid.append(last_one[:2*len(task_ids)])\n",
    "    sequences.append(temp_seq)\n",
    "#pp.pprint(sequences[0])\n",
    "length_interaction_vector = 2*(len(task_ids)+len(ccssm_labels)) #length of interaction vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "max_seqlen = max(seqlen)\n",
    "print(max_seqlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding the sequences according to maximum sequence length. Making padded sequences of shape: number of students, maximum sequence length, length of interaction vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = np.zeros(shape=(len(student_vectors),max_seqlen,length_interaction_vector),dtype=float)\n",
    "for i in range(len(sequences)):\n",
    "    for j in range(len(sequences[i])):\n",
    "        padded_sequences[i][j] = sequences[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets. Will take random validation sets at the time of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1130, 186, 28)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = round((training_set_split+validation_set_split)*len(student_vectors))\n",
    "training_x = padded_sequences[:split]\n",
    "training_y = np.asarray(output_y)[:split]\n",
    "training_y_ccssm = np.asarray(output_y_ccssm)[:split] #for validation set\n",
    "training_y_taskid = np.asarray(output_y_taskid)[:split] #for validation set\n",
    "test_x = padded_sequences[split:]\n",
    "test_y = np.asarray(output_y)[split:]\n",
    "test_y_ccssm = np.asarray(output_y_ccssm)[split:]\n",
    "test_y_taskid = np.asarray(output_y_taskid)[split:]\n",
    "training_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, max_seqlen, length_interaction_vector]) #(<batch_size>, <max_time>, <num_features>)\n",
    "y = tf.placeholder(tf.float32, [None, length_interaction_vector]) #(<batch_size>, <num_features>)\n",
    "\n",
    "def dynamicRNN(x):\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units)\n",
    "    outputs, states = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    #transformation on outputs needed, otherwise auc=0\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    outputs = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "    out_size = length_interaction_vector\n",
    "    logit = tf.contrib.layers.fully_connected(outputs, out_size, activation_fn=None)\n",
    "    if tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "        outputs = tf.sigmoid(logit)\n",
    "    else:\n",
    "        outputs = tf.nn.sigmoid(logit)\n",
    "    return outputs\n",
    "\n",
    "#making predictions\n",
    "pred = dynamicRNN(x)\n",
    "if tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "    pred_task,pred_ccssm = tf.split_v(value=pred,size_splits=[2*len(task_ids),2*len(ccssm_labels)],split_dim=1)\n",
    "else:\n",
    "    pred_task,pred_ccssm = tf.split(value=pred,num_or_size_splits=[2*len(task_ids),2*len(ccssm_labels)],axis=1)\n",
    "    \n",
    "# Define loss and optimizer\n",
    "if tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, targets=y))\n",
    "else:\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "if(optimize_using == \"momentum\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(cost)\n",
    "elif (optimize_using == \"adagrad\"):\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model - use AUC to evaluate model\n",
    "if tf.VERSION == '0.12.1': #summit's tensorflow version API doc: https://www.tensorflow.org/versions/r0.12/api_docs/\n",
    "    auc,  opts = tf.contrib.metrics.streaming_auc(labels = test_y_taskid, predictions = pred_task, curve='ROC')\n",
    "    auc_ccssm,  opts_ccssm = tf.contrib.metrics.streaming_auc(labels = test_y_ccssm, predictions = pred_ccssm, curve='ROC')\n",
    "else:\n",
    "    auc,  opts = tf.metrics.auc(labels = test_y_taskid, predictions = pred_task, curve='ROC')\n",
    "    auc_ccssm,  opts_ccssm = tf.metrics.auc(labels = test_y_ccssm, predictions = pred_ccssm, curve='ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss= [0.9380963]\n",
      "Step 2, Loss= [0.93782026]\n",
      "Step 3, Loss= [0.93754596]\n",
      "Step 4, Loss= [0.9372898]\n",
      "Step 5, Loss= [0.9370236]\n",
      "Step 6, Loss= [0.9367578]\n",
      "Step 7, Loss= [0.9364927]\n",
      "Step 8, Loss= [0.9362311]\n",
      "Step 9, Loss= [0.93594396]\n",
      "Step 10, Loss= [0.93569267]\n",
      "Step 11, Loss= [0.9354228]\n",
      "Step 12, Loss= [0.93513983]\n",
      "Step 13, Loss= [0.93489236]\n",
      "Step 14, Loss= [0.93461674]\n",
      "Step 15, Loss= [0.93435353]\n",
      "Step 16, Loss= [0.9340883]\n",
      "Step 17, Loss= [0.9338316]\n",
      "Step 18, Loss= [0.9335591]\n",
      "Step 19, Loss= [0.9332989]\n",
      "Step 20, Loss= [0.933027]\n",
      "Optimization Finished!\n",
      "Testing auc for taskid: 0.0, 0.84063154\n",
      "Testing auc for ccssm: 0.0, 0.9028572\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x = training_x\n",
    "        batch_y = training_y\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            loss= sess.run([cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Loss= \" + str(loss))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate test auc\n",
    "    temp_auc_ccssm, temp_opts_ccssm = sess.run([auc_ccssm,  opts_ccssm], feed_dict={x: test_x, y: test_y})\n",
    "    temp_auc_taskid, temp_opts_taskid = sess.run([auc, opts], feed_dict={x: test_x, y: test_y})\n",
    "    print(\"Testing auc for taskid: \" + str(temp_auc_taskid) + \", \" + str(temp_opts_taskid))\n",
    "    print(\"Testing auc for ccssm: \" + str(temp_auc_ccssm) + \", \" + str(temp_opts_ccssm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
